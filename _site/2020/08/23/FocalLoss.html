<!DOCTYPE html>
<html>

<head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# article: http://ogp.me/ns/article#">
<meta charset="utf-8" />
<meta http-equiv='X-UA-Compatible' content='IE=edge'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>
<title>Khoa học dữ liệu</title>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
<!-- Style for main home page -->
<link rel="stylesheet" href="/assets/css/styles.css">
<link rel="stylesheet" href="/assets/css/styles_toc.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<script data-ad-client="ca-pub-4263248182804679" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 
<link href="https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300" rel="stylesheet">
<!-- <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet"> -->
<link href="https://fonts.googleapis.com/css?family=Roboto|Source+Sans+Pro" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Fira+Sans" rel="stylesheet">
<link rel="icon" type="image/jpg" href="assets/images/logo.jpg" sizes="32x32">
<link rel="canonical" href="https://phamdinhkhanh.github.io"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="author" content="Phạm Đình Khánh" />
<meta property="og:title" content="" />
<meta property="og:site_name" content="Khanh's blog" />
<meta property="og:url" content="https://phamdinhkhanh.github.io" />
<meta property="og:description" content="" />

<meta property="og:type" content="article" />
<meta property="article:published_time" content="" />


<meta property="article:author" content="Khanh" />
<meta property="article:section" content="" />

<link rel="alternate" type="application/atom+xml" title="Khanh's blog - Atom feed" href="/feed.xml" />
<style>
	
</style>
<!-- -- Import latext  -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
	skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
	inlineMath: [['$','$']]
  }
});
</script>

<!-- Google Analytics -->

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-89509207-1', 'auto');
// ga('send', 'pageview');
ga('send', 'pageview', {
'page': '/',
'title': ''
});
</script>


<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-KTCD8BX');</script>
<!-- End Google Tag Manager -->
</head>
<style>
body {
  padding: 0 7.5%;
}
</style>

<body>
	<div id="fb-root"></div>
	<!-- <script>(function(d, s, id) { -->
	  <!-- var js, fjs = d.getElementsByTagName(s)[0]; -->
	  <!-- if (d.getElementById(id)) return; -->
	  <!-- js = d.createElement(s); js.id = id; -->
	  <!-- js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.9"; -->
	  <!-- fjs.parentNode.insertBefore(js, fjs); -->
	<!-- }(document, 'script', 'facebook-jssdk'));</script> -->
	<br>
	<div content = "container">
		<div class="row">
			<div class="col-md-2 hidden-xs hidden-sm">
				<a  href="/">
					<img width="100%" style="padding-bottom: 3mm;" src="/assets/images/img.jpg" /> </a>
				<br>
				<nav>
					<div class="header">Latest</div>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/08/23/FocalLoss.html">Bài 47 - Focal Loss trong RetinaNet</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/08/13/ModelMetric.html">Bài 46 - Đánh giá mô hình phân loại trong ML</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/08/09/ConditionalGAN.html">Bài 45 - Conditional GAN (CGAN)</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/07/25/GAN_Wasserstein.html">Bài 44 - Model Wasserstein GAN (WGAN)</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/07/13/GAN.html">Bài 43 - Model GAN</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/06/20/Unet.html">Bài 42 - Thực hành Unet</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/06/18/DeepLab.html">Bài 41 - DeepLab Sentiment Segmentation</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/06/10/ImageSegmention.html">Bài 40 - Image Segmentation</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/06/04/PhoBERT_Fairseq.html">Bài 39 - Thực hành ứng dụng BERT</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/05/31/CNNHistory.html">Bài 38 - Các kiến trúc CNN hiện đại</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/05/28/TransformerThemDauTV.html">Bài 37 - Transformer thêm dấu Tiếng Việt</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/05/23/BERTModel.html">Bài 36 - BERT model</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/05/05/MultitaskLearning_MultiBranch.html">Bài 35 - Multitask Learning - Multi Branch</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/04/22/MultitaskLearning.html">Bài 34 - Multitask Learning</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/04/15/TransferLearning.html">Bài 33 - Phương pháp Transfer Learning</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/04/09/TensorflowDataset.html">Bài 32 - Kĩ thuật tensorflow Dataset</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/04/03/AWS.html">Bài 31 - Amazon Virtual Machine Deep Learning</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/28/deployTensorflowJS.html">Bài 30 - Xây dựng Web AI trên tensorflow js</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/23/FlaskRestAPI.html">Bài 29 - Xây dựng Flask API cho mô hình deep learning</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/21/faceNet.html">Bài 28 - Thực hành training Facenet</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/12/faceNetAlgorithm.html">Bài 27 - Mô hình Facenet trong face recognition</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/10/DarknetGoogleColab.html">Bài 26 - Huấn luyện YOLO darknet trên google colab</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/09/DarknetAlgorithm.html">Bài 25 - YOLO You Only Look Once</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/02/17/ImbalancedData.html">Bài 24 - Mất cân bằng dữ liệu (imbalanced dataset)</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/02/11/NARSyscom2015.html">Bài 23 - Neural Attentive Session-Based Recommendation</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/01/17/ScoreCard.html">Bài 22 - Scorecard model</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/01/06/ImagePreprocessing.html">Bài 21 - Tiền xử lý ảnh OpenCV</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/12/26/Sorfmax_Recommendation_Neural_Network.html">Bài 20 - Recommendation Neural Network</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/12/12/ARIMAmodel.html">Bài 19 - Mô hình ARIMA trong time series</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/12/02/DeepLearningLayer.html">Bài 18 - Các layers quan trọng trong deep learning</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/11/22/HOG.html">Bài 17 - Thuật toán HOG (Histrogram of oriented gradient)</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/11/08/RFMModel.html">Bài 16 - Model RFM phân khúc khách hàng</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/11/04/Recommendation_Compound_Part1.html">Bài 15 - collaborative và content-based filtering</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/10/22/googleHeatmap.html">Bài 14 - Biểu đồ trên Google Map</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/10/05/SSDModelObjectDetection.html">Bài 13 - Model SSD trong Object Detection</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/09/29/OverviewObjectDetection.html">Bài 12 - Các thuật toán Object Detection</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/09/16/VisualizationPython.html">Bài 11 - Visualization trong python</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/09/08/LDATopicModel.html">Bài 10 - Thuật toán LDA - Xác định Topic</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/08/25/PyTorch_Torchtext_Tutorial.html">Bài 9 - Pytorch - Buổi 3 - torchtext module NLP</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/08/22/convolutional-neural-network.html">Bài 8 - Convolutional Neural Network</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/08/19/CorrectSpellingVietnamseTonePrediction.html">Bài 7 - Pytorch - Buổi 2 - Seq2seq model correct spelling</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/08/10/PytorchTurtorial1.html">Bài 6 - Pytorch - Buổi 1 - Làm quen với pytorch</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/07/15/PySparkSQL.html">Bài 5 - Model Pipeline - SparkSQL</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/06/18/AttentionLayer.html">Bài 4 -  Attention is all you need</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/05/10/Hypothesis_Statistic.html">Apenddix 1 - Lý thuyết phân phối và kiểm định thống kê</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/04/29/ModelWord2Vec.html">Bài 3 - Mô hình Word2Vec</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/04/22/Ly_thuyet_ve_mang_LSTM.html">Bài 2 - Lý thuyết về mạng LSTM part 2</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/01/07/Ky_thuat_feature_engineering.html">Bài 1 - Kĩ thuật feature engineering</a></li>
					
				</nav>
			</div>
			<div class="col-md-8 col-xs-12" style="z-index:1">
				<nav class="navbar navbar-inverse" style="background-color: #046897">
					<div class = "container-fluid">
						<div class = "navbar-header>
							<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
								<span class="icon-bar"></span>
								<span class="icon-bar"></span>
								<span class="icon-bar"></span>
							</button>
							<a class="navbar-brand" href="/">
								<span style="color:#FFF">Khoa học dữ liệu - Khanh's blog</span>
							</a>
						</div>
						<br>
						<br>
						<div class="collapse navbar-collapse navbar-right" id="myNavbar">
							<ul class="nav navbar-nav">
								<li><a href="/home"><span style="color: #fff"> Home</span></a></li>
								<li><a href="/about"><span style="color: #fff"> About</span></a></li>
								<!-- <li><a href="/da"><span style="color: #fff">Data Analytics</span></a></li> -->
								<!-- <li><a href="/cv"><span style="color: #fff">Computer Vision</span></a></li> -->
								<!-- <li><a href="/nlp"><span style="color: #fff">NLP</span></a></li> -->
								<!-- <li><a href="/code"><span style="color: #fff">Code</span></a></li> -->
								<li><a href="/book"><span style="color: #fff">Book</span></a></li>
							</ul>
						</div>
					</div>
				</nav>
				<div class="PageNavigation">
				</div>
				<h1 itemprop="name" class="post-title"></h1>
				<div id="bootstrap-overrides">
					<div>
<h2><p class="post-link" style="text-align: left; color: #204081; font-weight: bold">Bài 47 - Focal Loss trong RetinaNet</p></h2> 
<strong>23 Aug 2020 - phamdinhkhanh</strong>
</div>
<br/>
<div id="toc"></div>
<h1 id="1-focal-loss-function">1. Focal Loss Function</h1>

<p>Trong bài báo được trình bày vào tháng 1, 2018 tựa đề <a href="https://arxiv.org/pdf/1708.02002.pdf">Focal Loss for Dense Object Detection</a>, nhóm tác giả <code class="highlighter-rouge">Tsung-Yi Lin, Priya Goyal, ...</code> của FAIR (Facebook AI research) đã công bố một hàm loss function mới mang tính đột phá trong việc cải thiện hiệu xuất của lớp mô hình one-stage detector trong object detection.</p>

<p>Dựa trên nhận định rằng mất cân bằng dữ liệu giữa các nhóm foreground-background là nguyên nhân chính dẫn tới sự kém hiệu quả, xin trích dẫn :</p>

<p><code class="highlighter-rouge">We discover that the extreme foreground-background class imbalance encountered
during training of dense detectors is the central cause</code></p>

<p>Nhóm tác giả đã đề xuất một sự điều chỉnh trong hàm cross entropy loss để giải quyết triệt để ảnh hưởng của mất cân bằng dữ liệu. Cụ thể về phương pháp mà các tác gỉa đã áp dụng như thế nào? Hãy cùng tôi khám phá qua bài viết này.</p>

<h1 id="2-cross-entropy-loss">2. Cross Entropy Loss</h1>

<p>Chúng ta đã được làm quen với hàm <a href="https://phamdinhkhanh.github.io/2020/07/25/GAN_Wasserstein.html#2-cross-entropy">cross entropy</a>. Xin nhắc lại
giá trị cross entropy đối với phân phối xác suất thực tế $\mathbf{p}$ và phân phối xác suất dự báo $\mathbf{q}$:</p>

<script type="math/tex; mode=display">CE(\mathbf{p}, \mathbf{q}) \triangleq \mathbf{H}(\mathbf{p}, \mathbf{q}) = -\sum_{i=1}^{C}p_i\log({q_i})</script>

<p>Tại các nhãn bằng 0 giá trị đóng góp vào loss function bằng 0. Do đó cross entropy có thể viết lại thành :</p>

<script type="math/tex; mode=display">CE(\mathbf{p}, \mathbf{q}) = -\log(q_i), \text{with} ~ p_i=1</script>

<p>Trong cross entropy ta thấy rằng vai trò đóng góp vào loss function của các class cùng bằng -$\log(p_i)$. Khi xảy ra hiện tượng mất cân bằng, chúng ta muốn rằng mô hình sẽ dự báo chuẩn hơn đối với những class thiểu số. Do đó cần một hàm loss function hiệu quả hơn, có thể điều chỉnh được giá trị phạt lớn hơn đối với nhóm thiểu số. Mục đích là để hạn chế dự báo sai nhóm thiểu số vì nếu dự báo sai nhóm thiểu số thì hàm loss function sẽ trở nên lớn hớn.</p>

<h1 id="3-hàm-balanced-cross-entropy">3. Hàm balanced cross entropy</h1>

<p>Các tự nhiên nhất là áp dụng trọng số bằng nghịch đảo tần suất nhãn vào cross entropy. Hàm loss function mới được gọi là <em>balanced cross entropy</em>:</p>

<script type="math/tex; mode=display">BCE(\mathbf{p}, \mathbf{q}) = -\alpha_i\log(q_i),~~~ \text{with} ~ p_i=1</script>

<p>Trong đó $\alpha_i = \frac{1}{f_i+\epsilon}$, $f_i$ là tần suất của class $i$. Chúng ta cộng thêm $\epsilon$ <em>dương rất nhỏ</em> để tránh mẫu bằng 0. Với hàm loss function này, các classes xuất hiện ít hơn thì có giá trị tác động tới loss function lớn hơn.</p>

<p>Hàm <em>balanced cross entropy</em> là hàm số cân bằng được tỷ lệ phân phối của mẫu. Nhưng nó chưa thực sự thay đổi được gradient descent của loss function. Trong khi mô hình được huấn luyện trên mẫu mất cân bằng trầm trọng có giá trị gradient descent chịu ảnh hưởng phần lớn bởi class chiếm đa số. Do đó chúng ta cần một sự điều chỉnh triệt để hơn giúp gia tăng ảnh hưởng của nhóm thiểu số lên gradient descent. Đó chính là hàm <em>focal loss</em>, một hàm số tiếp tục kế thừa <em>balanced cross entropy</em> và điều chỉnh được <em>gradient descent</em>.</p>

<h1 id="4-sự-ra-đời-của-focal-loss">4. Sự ra đời của focal loss</h1>

<p>Focal loss là hàm loss function lần đầu được giới thiệu trong RetinaNet. Hàm loss function này đã chứng minh được tính hiệu quả trong các bài toán object detection. Đây là lớp bài toán có sự mất cân bằng nghiêm trọng giữa hai class positive (các bounding box có chứa object) và negative (các bounding box chứa object). Thường thì <em>negative</em> có số lượng lớn hơn <em>positive</em> rất nhiều. Lấy ví dụ như hình bên dưới :</p>

<p><img src="/assets/images/20200823_FocalLoss/pic1.png" class="largepic" /></p>

<p>Chỉ có 4 bounding box thuộc positive (đường viền in đậm), các trường hợp còn lại thuộc nhóm negative.</p>

<p>Do đó nếu áp dụng loss function là hàm cross entropy sẽ giảm độ chính xác khi dự báo các bounding box có chứa object. Trong bài báo <a href="https://arxiv.org/pdf/1708.02002.pdf">Focal Loss for Dense Object Detection</a> tác giả đã giới thiệu hàm Focal Loss dựa trên hai tham số là $\alpha, \gamma$ như sau:</p>

<script type="math/tex; mode=display">FP(\mathbf{p}, \mathbf{q}) = -\alpha_i (1-q_i)^{\gamma} \log(q_i), ~~~ \text{with} ~ p_i=1</script>

<p>Chúng ta thường chọn giá trị $\gamma \in [0, 5]$.</p>

<p>Ta thấy hàm <em>focal loss</em> chỉ thêm nhân tử $(1-q_i)^{\gamma}$ so với công thức của <em>balanced cross entropy</em>. Tuy nhiên nhân tử này lại có tác dụng rất lớn trong việc điều chỉnh <em>ảnh hưởng của nhãn</em> lên đồng thời <em>loss function</em> và <em>gradient descent</em>. Thật vậy, xét hai trường hợp dễ dự báo và khó dự báo :</p>

<ul>
  <li>
    <p>Dễ dự báo: Chúng ta thấy rằng mô hình huấn luyện trên mẫu mất cân bằng thường dự báo chính xác các mẫu đa số. Những trường hợp này được gọi là dễ dự báo. Xác suất $q_i$ của của các trường hợp dễ dự báo có xu hướng cao hơn. Do đó $(1-p_t)^{\gamma}$ có xu hướng rất nhỏ và dường như không tác động lên loss function đáng kể.</p>
  </li>
  <li>
    <p>Khó dự báo: Trường hợp khó dự báo thì $q_i$ là một giá trị nhỏ hơn. Do đó độ lớn tác động của nó lên loss function là $(1-q_i)^{\gamma}$ sẽ gần bằng 1. Mức độ tác động này lớn hơn rất nhiều lần so với trường hợp dễ dự báo. Cụ thể hơn, nếu trường hợp dễ dự báo có $p_i = 0.9$ và khó dự báo có $p_i = 0.1$ thì tỷ lệ chênh lệch của đóng góp vào loss function khi $\gamma=2$ sẽ là:</p>
  </li>
</ul>

<script type="math/tex; mode=display">\frac{(1-0.1)^2}{(1-0.9)^2} = \frac{0.9^2}{0.1^2} = 81</script>

<p>Tỷ lệ này sẽ còn lớn hơn nữa nếu tăng $\gamma$ hoặc giá trị của $p_i$ đối với trường hợp dễ dự báo càng gần 1 và khó dự báo càng gần 0.</p>

<h2 id="41-tác-động-tới-đạo-hàm">4.1. Tác động tới đạo hàm</h2>

<p>Phần phân tích sắp tới đây sẽ khá nặng về toán, bạn đọc không quan tâm nhiều tới toán có thể bỏ qua và chỉ xem 2 note kết luận ở cuối chương.</p>

<p>Ta có đạo hàm của <em>focal loss</em> như sau:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin {eqnarray}\frac{\delta FP(\mathbf{p}, \mathbf{q})}{\delta q_i} & = &\frac{-\alpha_i \delta [ (1-q_i)^{\gamma} \log(q_i)]}{\delta q_i} \\
& = & \frac{-\alpha_i[-\gamma(1-q_i)^{\gamma-1}\log(q_i)+\frac{(1-q_i)^\gamma}{q_i}]}{\delta q_i} \\
& = & \frac{\alpha_i(1-q_i)^\gamma [\frac{\gamma\log(q_i)}{1-q_i}-\frac{1}{q_i}]}{\delta q_i} \\
& \triangleq & \frac{\alpha_i(1-q_i)^\gamma g(x)}{\delta q_i}
\end{eqnarray} %]]></script>

<p>Dòng thứ 4 ở biến đổi trên ta đã đặt $g(x) = [\frac{\gamma\log(q_i)}{1-q_i}-\frac{1}{q_i}]$.</p>

<p>Mặt khác ta có một bất đẳng thức hết sức quan trọng đối với $\log (x)$ khi $x \in (0, 1]$. Chắc hẳn bạn còn nhớ:</p>

<p><script type="math/tex">\log (x) \geq \beta(\frac{1}{x}-1)</script> 
với $\forall \beta \leq -1$.</p>

<p>Note: Hàm $\log$ tôi ký hiệu ở trên là logarith cơ số tự nhiên $e$ chứ không phải cơ số 10.</p>

<p>Chứng minh bất đẳng thức này bằng khảo sát sự biến thiên của đạo hàm khá đơn giản như sau:</p>

<p>Đặt $f(x) = \log(x)-\beta(\frac{1}{x}-1)$ suy ra:</p>

<script type="math/tex; mode=display">f'(x) = \frac{1}{x}+\frac{\beta}{x^2}=\frac{x+\beta}{x^2} \leq \frac{1+\beta}{x^2} \leq 0</script>

<p>Như vậy $f(x)$ nghịch biến trên $(0, 1]$. Từ đó suy ra cực tiểu $\min_{x \in (0, 1]} f(x) = f(1) = 0$.</p>

<p>Tương tự ta cũng chứng minh được bất đẳng thức sau:</p>

<p><script type="math/tex">\log (x) \leq \beta(\frac{1}{x}-1)</script> 
với $\forall \beta \geq 0$.</p>

<p>Chúng ta có thể chứng minh bất đẳng thức này bằng đạo hàm khá dễ dàng. Phần chứng minh xin dành cho bạn đọc như một bài tập. Đẳng thức xảy ra tại $x = 1$.</p>

<p>Như vậy nếu chọn $\beta_1 \leq -1$  và $\beta_2 \geq 0$ là hai giá trị bất kỳ ta có:</p>

<script type="math/tex; mode=display">g(x) = \frac{\gamma\log(q_i)}{1-q_i}-\frac{1}{q_i} \geq \frac{\gamma\beta_1(\frac{1}{q_i}-1)}{1-q_i}-\frac{1}{q_i} = \gamma\beta_1-1 = C_1</script>

<p>Và đồng thời :</p>

<script type="math/tex; mode=display">g(x) = \frac{\gamma\log(q_i)}{1-q_i}-\frac{1}{q_i} \leq \frac{\gamma\beta_2(\frac{1}{q_i}-1)}{1-q_i}-\frac{1}{q_i} = \gamma\beta_2-1 = C_2</script>

<p>Điều này chứng tỏ giá trị của $g(x)$ bị chặn trong đoạn $[C_1, C_2]$. Đây làm một nhận định hết sức quan trọng vì điều đó chứng tỏ rằng độ lớn gradient của <em>focal loss</em> sẽ phần lớn phụ thuộc vào $(1-q_i)^{\gamma}$.</p>

<p>Bên dưới ta sẽ xét 2 trường hợp :</p>

<ul>
  <li>
    <p>Dễ dự báo: Giá trị $(1-p_t)^{\gamma}$ có xu hướng rất nhỏ và do đó ảnh hưởng của gradient descent của các trường hợp dễ dự báo không đáng kể.</p>
  </li>
  <li>
    <p>Khó dự báo: $(1-q_i)^{\gamma}$ sẽ gần bằng 1. Mức độ tác động lên gradient descent lớn hơn rất nhiều lần so với trường hợp dễ dự báo.</p>
  </li>
</ul>

<p>Như vậy bạn đọc đã thấy được vai trò của focal loss trong việc điều chỉnh ảnh hưởng của phân phối mẫu lên <em>gradient descent</em> rồi chứ ?</p>

<p>Đó chính là điều kỳ diệu giúp cho các mô hình object detection sử dụng focal loss có độ chính xác cao hơn.</p>

<h2 id="42-đồ-thị-của-focal-loss">4.2. Đồ thị của focal loss</h2>

<p>Trường hợp $\gamma=0$ ta thấy hàm focal loss chính là balanced cross entropy. Trong quá trình xây dựng mô hình thì chúng ta có thể tunning giá trị của $\gamma$ và $\alpha$ để tìm ra bộ siêu tham số tốt nhất cho mô hình của mình. Như trong bài báo thì tác giả đã tìm được $\gamma = 2$ và $\alpha=0.25$ là bộ siêu tham số tốt nhất trên dữ liệu COCO.</p>

<p>Bên dưới chúng ta cùng visualize một số trường hợp của focal loss.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre></td><td class="rouge-code"><pre>import numpy as np
import matplotlib.pyplot as plt

alpha = 1
gammas = [0, 0.5, 1, 2, 5]
p = np.linspace(0, 1, 200)[1:]

def _focal_loss(p, gamma, alpha = 1):
  loss = -(1-p)**gamma*np.log(p)
  return loss

plt.figure(figsize=(16, 8))

for gamma in gammas:
  loss = _focal_loss(p, gamma = gamma)
  if gamma == 0:
    label = 'cross entropy'
  else:
    label = 'gamma = {}'.format(gamma)
  plt.plot(p, loss, label = label)

plt.title('Focal Loss function with gamma', fontsize=20)
plt.xlabel('pi', fontsize=12)
plt.ylabel('Loss values', fontsize=12)
plt.legend()

</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="/assets/images/20200823_FocalLoss/FocalLoss_6_1.png" class="largepic" /></p>

<p>Ta nhận thấy rằng cross entropy có sự khác biệt giữa giá trị đóng góp vào loss function của các trường hợp xác suất cao (dễ dự báo ) và xác suất thấp (khó dự báo ) là thấp nhất. Khi $\gamma$ càng lớn thì tỷ lệ khác biệt của loss function giữa các trường hợp này càng lớn thể hiện qua đường cong càng trũng xuống.</p>

<h1 id="5-retina-net">5. Retina net</h1>

<p>Ở phần này chúng ta sẽ tìm hiểu về kiến trúc retina net trong lớp bài toán object detection. Nếu bạn đọc chưa biết object detection là gì có thể xem lại các bài <a href="https://phamdinhkhanh.github.io/2019/09/29/OverviewObjectDetection.html">Bài 12: Object detection</a>, <a href="https://phamdinhkhanh.github.io/2019/10/05/SSDModelObjectDetection.html">Bài 13: SSD</a>, <a href="https://phamdinhkhanh.github.io/2020/03/09/DarknetAlgorithm.html">Bài 25: YOLO</a>. Retina net là một mô hình giải quyết được <code class="highlighter-rouge">vấn đề mất cân bằng trong phân phối giữa foreground và background trong các bài toán one-stage detection</code> bằng cách sử dụng hàm focal loss. Như chúng ta ta đã tìm hiểu ở chương trước, focal loss sẽ giảm thiểu mất mát của những trường hợp dễ dự báo (là foreground) và do đó tập trung hơn vào những trường hợp khó dự báo. Nhờ đó cải thiện được độ chính xác.</p>

<p>Kiến trúc trong bài báo gốc tác giả giới thiệu gồm hai phase:</p>

<ul>
  <li>Phase 1: là một feature extractor kết hợp giữa Resnet + FPN, có tác dụng trích lọc đặc trưng và trả về các feature map. Mạng FPN (Featuer Pyramid Network) sẽ tạo ra một multi-head  dạng kim tự tháp.</li>
</ul>

<p><img src="/assets/images/20200823_FocalLoss/pic2.png" class="largepic" /></p>

<p><strong>Hình 1:</strong> Kiến trúc FPN. Bao gồm hai nhánh là Bottom-Up bên trái và Top-Down bên phải.</p>

<ul>
  <li>
    <p>Nhánh Bottom-Up là một mạng Convolutional Neural Network (trong bài này là Restnet) giúp tạo ra pyramid level các feature map theo kích thước giảm dần. Những feature map này sẽ được kết hợp với feature map cùng cấp nhánh Top Down.</p>
  </li>
  <li>
    <p>Nhánh Top-Down: Là một mạng upscaling các feature map theo kích thước nhân 2. Như vậy mỗi một level của nhánh bên phải sẽ kết hợp với một level ở nhánh bên trái có cùng kích thước thông qua một phép cộng element-wise additional (cộng các phần tử ở cùng vị trí với nhau). Trước khi thực hiện phép cộng thì level nhánh bên trái được tích chập với feature map <code class="highlighter-rouge">1x1</code> để giảm thiểu độ sâu channel. Mỗi một phép cộng sẽ trả ra một <em>merge map</em> như các ô predict trong hình.</p>
  </li>
</ul>

<p>Như vậy chúng ta thấy kiến trúc của mạng Retina net cũng hoàn toàn đơn giản phải không nào ?</p>

<p><img src="/assets/images/20200823_FocalLoss/pic3.png" class="gigantic" /></p>

<p><strong>Hình 2:</strong> Kiến trúc của RetinaNet, <em>Source</em>: <a href="https://arxiv.org/pdf/1708.02002.pdf">Figure 3: Focal Loss for Dense Object Detection</a></p>

<ul>
  <li>
    <p>Dự báo: Các <em>merge map</em> được sử dụng để dự báo cho các <code class="highlighter-rouge">class+box subnets</code>. Kiến trúc của một <code class="highlighter-rouge">class+box subnets</code> gồm hai nhãnh như hình vẽ.</p>

    <ul>
      <li>
        <p>Nhánh phía trên là <code class="highlighter-rouge">class subnet</code> để dự báo phân phối xác suất của các classes. Bạn để ý rằng số lượng channel ở output của nhánh này là $KA$ chính là số lượng classes x số lượng Anchor.</p>
      </li>
      <li>
        <p>Nhánh phía dưới là <code class="highlighter-rouge">box subnet</code> dự báo tọa độ $(c_x, c_y, w, h)$. Do đó đầu ra của chúng có số lượng channel là $4A = 4 \times \text{Number_Anchor}$.</p>
      </li>
    </ul>
  </li>
</ul>

<p>Các dự báo class và box được thực hiện trên từng level với scale khác nhau nhằm phát hiện được vật thể ở đa dạng kích thước.</p>

<p>Kết quả của mô hình RetinaNet</p>

<p><img src="/assets/images/20200823_FocalLoss/pic4.png" class="largepic" /></p>

<p><em>Source</em>: <a href="https://arxiv.org/pdf/1708.02002.pdf">Table 2: Focal Loss for Dense Object Detection</a></p>

<p>So sánh với các mô hình One-stage detector lúc bấy giờ thì RetinaNet có kết quả tốt hơn <em>YOLOv2, SSD513, DSSD513</em>. Xét trên độ chính xác AP, RetinaNet giúp cải thiện tới 6 điểm (33.2 -&gt; 39.1) so với mô hình tốt kém hơn là <em>DSSD513</em>, cả hai cùng sử dụng backbone ResNet101. Đây là kết quả khá tốt.</p>

<h2 id="51-xây-dựng-mô-hình">5.1. Xây dựng mô hình</h2>

<p>Đã có nhiều code huấn luyện mô hình RetinaNet được chia sẻ. Mình có thể liệt kê một số code tốt :</p>

<ul>
  <li>
    <p>Nếu bạn dùng tensorflow: <a href="https://github.com/fizyr/keras-retinanet">keras-retinanet</a></p>
  </li>
  <li>
    <p>Code trên pytorch: <a href="https://github.com/yhenon/pytorch-retinanet">pytorch-retinanet</a></p>
  </li>
</ul>

<p>Để khởi tạo được kiến trúc này chúng ta quan tâm tới phần quan trọng nhất đó là kiến trúc ResNet+FPN. Mình sẽ hướng dẫn các bạn phần này trên keras. Source code được trích từ <a href="https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/models/retinanet.py">retinanet</a> :</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
</pre></td><td class="rouge-code"><pre>from tensorflow import keras


class UpsampleLike(keras.layers.Layer):
    """ Một Keras Custom Layer có tác dụng reshape kích thước source tensor về cùng shape với target tensor.
    """

    def call(self, inputs, **kwargs):
        """inputs: list [source, target]
        """
        source, target = inputs
        target_shape = keras.backend.shape(target)
        if keras.backend.image_data_format() == 'channels_first':
            source = backend.transpose(source, (0, 2, 3, 1))
            output = backend.resize_images(source, (target_shape[2], target_shape[3]), method='nearest')
            output = backend.transpose(output, (0, 3, 1, 2))
            return output
        else:
            return backend.resize_images(source, (target_shape[1], target_shape[2]), method='nearest')

    def compute_output_shape(self, input_shape):
        if keras.backend.image_data_format() == 'channels_first':
            return (input_shape[0][0], input_shape[0][1]) + input_shape[1][2:4]
        else:
            return (input_shape[0][0],) + input_shape[1][1:3] + (input_shape[0][-1],)


def __create_pyramid_features(backbone_layers, pyramid_levels, feature_size=256):
    """
    Khởi tạo các FPN layers dựa trên các backbone features.
    Args
        backbone_layers: Một dictionary chứa các feature level C3, C4, C5 từ backbone.
        pyramid_levels: Các Pyramid levels được sử dụng.
        feature_size : Độ sâu của các feature level. Mặc định 256.
    Returns
        output_layers : Một từ điển gồm các feature levels.

    """

    output_layers = {}
  
    ## Step 1: upsample C5 để thu được P5 như trong FPN paper 
    P5           = keras.layers.Conv2D(feature_size, kernel_size=1, strides=1, padding='same', name='C5_reduced')(backbone_layers['C5'])
    P5_upsampled = layers.UpsampleLike(name='P5_upsampled')([P5, backbone_layers['C4']])
    P5           = keras.layers.Conv2D(feature_size, kernel_size=3, strides=1, padding='same', name='P5')(P5)
    output_layers["P5"] = P5

    ## Step 2: merge P5_upsampled + C4
    P4           = keras.layers.Conv2D(feature_size, kernel_size=1, strides=1, padding='same', name='C4_reduced')(backbone_layers['C4'])
    P4           = keras.layers.Add(name='P4_merged')([P5_upsampled, P4])
    P4_upsampled = layers.UpsampleLike(name='P4_upsampled')([P4, backbone_layers['C3']])
    P4           = keras.layers.Conv2D(feature_size, kernel_size=3, strides=1, padding='same', name='P4')(P4)
    output_layers["P4"] = P4

    ## Step 3: merge P4_upsampled + C3
    P3 = keras.layers.Conv2D(feature_size, kernel_size=1, strides=1, padding='same', name='C3_reduced')(backbone_layers['C3'])
    P3 = keras.layers.Add(name='P3_merged')([P4_upsampled, P3])
    if 'C2' in backbone_layers and 2 in pyramid_levels:
        P3_upsampled = layers.UpsampleLike(name='P3_upsampled')([P3, backbone_layers['C2']])
    P3 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=1, padding='same', name='P3')(P3)
    output_layers["P3"] = P3

    ## Step 4: merge P3_upsampled + C2
    if 'C2' in backbone_layers and 2 in pyramid_levels:
        P2 = keras.layers.Conv2D(feature_size, kernel_size=1, strides=1, padding='same', name='C2_reduced')(backbone_layers['C2'])
        P2 = keras.layers.Add(name='P2_merged')([P3_upsampled, P2])
        P2 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=1, padding='same', name='P2')(P2)
        output_layers["P2"] = P2

    # "P6 thu được thông qua tích chập Conv2D(kernel_size=3, stride=2) trên C5"
    if 6 in pyramid_levels:
        P6 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=2, padding='same', name='P6')(backbone_layers['C5'])
        output_layers["P6"] = P6

    # "P7 áp dụng ReLU sau một tích chập Conv2D(kernel_size=3, stride=2) trên P6"
    if 7 in pyramid_levels:
        if 6 not in pyramid_levels:
            raise ValueError("P6 is required to use P7")
        P7 = keras.layers.Activation('relu', name='C6_relu')(P6)
        P7 = keras.layers.Conv2D(feature_size, kernel_size=3, strides=2, padding='same', name='P7')(P7)
        output_layers["P7"] = P7

    return output_layers
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Mỗi một bước trong hàm <code class="highlighter-rouge">__create_pyramid_features()</code> chúng ta sẽ thực hiện lần lượt các thao tác:</p>

<ul>
  <li>Tích chập Conv2D với kích thước <code class="highlighter-rouge">1x1</code> với level $C_i$ bên nhánh Bottom-Up (mạng ResNet) để giảm độ sâu về 256.</li>
  <li>Khởi tạo level $P_{i-1}$ tiếp theo bên nhánh Top-Down (mạng FPN) có kích thước gấp đôi bằng cách upsampling lên gấp đôi kích thước level $P_i$ liền trước. Chính xác hơn, trong code trên thì chúng ta upsample sao cho bằng kích thước của $C_{i-1}$.</li>
  <li>Cộng elementwise additional hai kết quả từ upsampling và tích chập giảm độ sâu.</li>
</ul>

<p>Chúng ta có thể có option đó là tạo thêm level thấp nhất bắt đầu từ $P_2$ và tạo thêm level cao nhất bắt đầu từ $P_7$ tùy vào khai báo trong pyramid_levels.</p>

<p>Chi tiết về quá trình huấn luyện các bạn có thể xem tại <a href="https://github.com/fizyr/keras-retinanet">REAME-keras-retinanet</a>.</p>

<h1 id="6-kết-luận">6. Kết luận</h1>

<p>Như vậy qua bài này các bạn đã được nắm bắt thêm một thuật toán mới được áp dụng trong object detection đó là RetinaNet. Điểm khác biệt tạo ra thành công của RetinaNet đó là áp dụng mạng FPN, một multi-scale levels map và áp dụng Focal Loss để tăng cường độ chính xác khi dự báo vị trí chứa object. Mình tin rằng đây là ý tưởng độc đáo và nó sẽ còn tiếp tục được kế thừa trong những lớp mô hình SOTA về sau. Cuối cùng không thể thiếu là những tài liệu tham khảo mà mình đã sử dụng để viết bài viết này.</p>

<h1 id="7-tài-liệu">7. Tài liệu</h1>

<ol>
  <li><a href="https://arxiv.org/pdf/1708.02002.pdf">Focal Loss for Dense Object Detection</a></li>
  <li><a href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4">Review retinanet focal loss object detection - Sik-Ho Tsang</a></li>
  <li><a href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610">Review: FPN — Feature Pyramid Network (Object Detection) - Sik-Ho Tsang</a></li>
  <li><a href="https://developers.arcgis.com/python/guide/how-retinanet-works/">How retinanet works</a></li>
  <li><a href="https://leimao.github.io/blog/Focal-Loss-Explained/">Focal Loss Explained - LeiMao</a></li>
  <li><a href="https://deep-learning-study-note.readthedocs.io/en/latest/Part%202%20(Modern%20Practical%20Deep%20Networks)/12%20Applications/Computer%20Vision%20External/Focal%20Loss%20for%20Dense%20Object%20Detection.html">Focal Loss for Dense Object Detection</a></li>
  <li><a href="https://medium.com/@14prakash/the-intuition-behind-retinanet-eb636755607d">The intuition behind retinanet - Prakash Jay
</a></li>
  <li><a href="https://keras.io/examples/vision/retinanet/">Object Detection with retinanet - Keras Tutorial</a></li>
</ol>

<script data-ad-client="ca-pub-4263248182804679" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<button onclick="topFunction()" id="myBtn" title="Go to top">Top</button>

<script src="/js/toc.js"></script>
<script src="/js/btnTop.js"></script>
<script type="text/javascript">
$(document).ready(function() {
    $('#toc').toc();
});
</script>
				</div>
			</div>
			<div class="col-md-2 hidden-xs hidden-sm">
				<a  href="/">
					<img width="100%" style="padding-bottom: 3mm;" src="/assets/images/logo.jpg" /> </a>
				<br>
				<nav>
					<div class="header">Khanh's site</div>
					<li><a style="text-align: left; color: #074B80"  href="https://www.facebook.com/TowardDataScience">phamdinhkhanh blog</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://www.facebook.com/groups/3235479620010379/">phamdinhkhanh AICode forum</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://kaggle.com/phamdinhkhanh">phamdinhkhanh kaggle</a></li>
					<li><a style="text-align: left; color: #074B80"  href="http://rpubs.com/phamdinhkhanh">phamdinhkhanh rpub</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://github.com/phamdinhkhanh">phamdinhkhanh github</a></li>
					<br>
					<div class="header">other's site</div>
					<li><a style="text-align: left; color: #074B80"  href="https://www.facebook.com/groups/machinelearningcoban/">machine learning cơ bản facebook</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://forum.machinelearningcoban.com/">machine learning cơ bản forum</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://machinelearningmastery.com">machine learning mastery</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://viblo.asia">viblosia</a></li>
					<br>
					<div class="header">Khóa học</div>
					<li><a style="text-align: left; color: #074B80"  href="http://web.stanford.edu/class/cs109/">Xác suất thống kê(Probability): CS109</a></li>
					<li><a style="text-align: left; color: #074B80"  href="http://web.stanford.edu/class/cs246/">Bigdata: CS246</a></li>
					<li><a style="text-align: left; color: #074B80"  href="http://cs231n.stanford.edu/">Computer vision cơ bản: CS231N</a></li>
					<li><a style="text-align: left; color: #074B80"  href="http://web.stanford.edu/class/cs224n/">Natural Language Processing: CS224N</a></li>
					<li><a style="text-align: left; color: #074B80"  href="http://web.stanford.edu/class/cs224w/">Khoá phân tích mạng lưới (analysis of network): CS224W</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://web.stanford.edu/class/cs20si/">Khóa học Tensorflow: CS20SI</a></li>
				</nav>
			</div>
		</div>
	</div>
	
</body>
</html>
