<!DOCTYPE html>
<html>

<head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# article: http://ogp.me/ns/article#">
<meta charset="utf-8" />
<meta http-equiv='X-UA-Compatible' content='IE=edge'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>
<title>Khoa học dữ liệu</title>
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
<!-- Style for main home page -->
<link rel="stylesheet" href="/assets/css/styles.css">
<link rel="stylesheet" href="/assets/css/styles_toc.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<script data-ad-client="ca-pub-4263248182804679" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 
<link href="https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300" rel="stylesheet">
<!-- <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet"> -->
<link href="https://fonts.googleapis.com/css?family=Roboto|Source+Sans+Pro" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Fira+Sans" rel="stylesheet">
<link rel="icon" type="image/jpg" href="assets/images/logo.jpg" sizes="32x32">
<link rel="canonical" href="https://phamdinhkhanh.github.io"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="author" content="Phạm Đình Khánh" />
<meta property="og:title" content="" />
<meta property="og:site_name" content="Khanh's blog" />
<meta property="og:url" content="https://phamdinhkhanh.github.io" />
<meta property="og:description" content="" />

<meta property="og:type" content="article" />
<meta property="article:published_time" content="" />


<meta property="article:author" content="Khanh" />
<meta property="article:section" content="" />

<link rel="alternate" type="application/atom+xml" title="Khanh's blog - Atom feed" href="/feed.xml" />
<style>
	
</style>
<!-- -- Import latext  -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
	skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
	inlineMath: [['$','$']]
  }
});
</script>

<!-- Google Analytics -->

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-89509207-1', 'auto');
// ga('send', 'pageview');
ga('send', 'pageview', {
'page': '/',
'title': ''
});
</script>


<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-KTCD8BX');</script>
<!-- End Google Tag Manager -->
</head>
<style>
body {
  padding: 0 7.5%;
}
</style>

<body>
	<div id="fb-root"></div>
	<!-- <script>(function(d, s, id) { -->
	  <!-- var js, fjs = d.getElementsByTagName(s)[0]; -->
	  <!-- if (d.getElementById(id)) return; -->
	  <!-- js = d.createElement(s); js.id = id; -->
	  <!-- js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.9"; -->
	  <!-- fjs.parentNode.insertBefore(js, fjs); -->
	<!-- }(document, 'script', 'facebook-jssdk'));</script> -->
	<br>
	<div content = "container">
		<div class="row">
			<div class="col-md-2 hidden-xs hidden-sm">
				<a  href="/">
					<img width="100%" style="padding-bottom: 3mm;" src="/assets/images/img.jpg" /> </a>
				<br>
				<nav>
					<div class="header">Latest</div>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/07/25/GAN_Wassteiner.html">Bài 44 - Model Wasserstein GAN (WGAN)</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/07/25/GAN_Wasserstein.html">Bài 44 - Model Wasserstein GAN (WGAN)</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/07/13/GAN.html">Bài 43 - Model GAN</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/06/20/Unet.html">Bài 42 - Thực hành Unet</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/06/18/DeepLab.html">Bài 41 - DeepLab Sentiment Segmentation</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/06/10/ImageSegmention.html">Bài 40 - Image Segmentation</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/06/04/PhoBERT_Fairseq.html">Bài 39 - Thực hành ứng dụng BERT</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/05/31/CNNHistory.html">Bài 38 - Các kiến trúc CNN hiện đại</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/05/28/TransformerThemDauTV.html">Bài 37 - Transformer thêm dấu Tiếng Việt</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/05/23/BERTModel.html">Bài 36 - BERT model</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/05/05/MultitaskLearning_MultiBranch.html">Bài 35 - Multitask Learning - Multi Branch</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/04/22/MultitaskLearning.html">Bài 34 - Multitask Learning</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/04/15/TransferLearning.html">Bài 33 - Phương pháp Transfer Learning</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/04/09/TensorflowDataset.html">Bài 32 - Kĩ thuật tensorflow Dataset</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/04/03/AWS.html">Bài 31 - Amazon Virtual Machine Deep Learning</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/28/deployTensorflowJS.html">Bài 30 - Xây dựng Web AI trên tensorflow js</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/23/FlaskRestAPI.html">Bài 29 - Xây dựng Flask API cho mô hình deep learning</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/21/faceNet.html">Bài 28 - Thực hành training Facenet</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/12/faceNetAlgorithm.html">Bài 27 - Mô hình Facenet trong face recognition</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/10/DarknetGoogleColab.html">Bài 26 - Huấn luyện YOLO darknet trên google colab</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/03/09/DarknetAlgorithm.html">Bài 25 - YOLO You Only Look Once</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/02/17/ImbalancedData.html">Bài 24 - Mất cân bằng dữ liệu (imbalanced dataset)</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/02/11/NARSyscom2015.html">Bài 23 - Neural Attentive Session-Based Recommendation</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/01/17/ScoreCard.html">Bài 22 - Scorecard model</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2020/01/06/ImagePreprocessing.html">Bài 21 - Tiền xử lý ảnh OpenCV</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/12/26/Sorfmax_Recommendation_Neural_Network.html">Bài 20 - Recommendation Neural Network</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/12/12/ARIMAmodel.html">Bài 19 - Mô hình ARIMA trong time series</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/12/02/DeepLearningLayer.html">Bài 18 - Các layers quan trọng trong deep learning</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/11/22/HOG.html">Bài 17 - Thuật toán HOG (Histrogram of oriented gradient)</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/11/08/RFMModel.html">Bài 16 - Model RFM phân khúc khách hàng</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/11/04/Recommendation_Compound_Part1.html">Bài 15 - collaborative và content-based filtering</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/10/22/googleHeatmap.html">Bài 14 - Biểu đồ trên Google Map</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/10/05/SSDModelObjectDetection.html">Bài 13 - Model SSD trong Object Detection</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/09/29/OverviewObjectDetection.html">Bài 12 - Các thuật toán Object Detection</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/09/16/VisualizationPython.html">Bài 11 - Visualization trong python</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/09/08/LDATopicModel.html">Bài 10 - Thuật toán LDA - Xác định Topic</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/08/25/PyTorch_Torchtext_Tutorial.html">Bài 9 - Pytorch - Buổi 3 - torchtext module NLP</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/08/22/convolutional-neural-network.html">Bài 8 - Convolutional Neural Network</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/08/19/CorrectSpellingVietnamseTonePrediction.html">Bài 7 - Pytorch - Buổi 2 - Seq2seq model correct spelling</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/08/10/PytorchTurtorial1.html">Bài 6 - Pytorch - Buổi 1 - Làm quen với pytorch</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/07/15/PySparkSQL.html">Bài 5 - Model Pipeline - SparkSQL</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/06/18/AttentionLayer.html">Bài 4 -  Attention is all you need</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/05/10/Hypothesis_Statistic.html">Apenddix 1 - Lý thuyết phân phối và kiểm định thống kê</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/04/29/ModelWord2Vec.html">Bài 3 - Mô hình Word2Vec</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/04/22/Ly_thuyet_ve_mang_LSTM.html">Bài 2 - Lý thuyết về mạng LSTM part 2</a></li>
					
					<li><a style="text-align: left; color: #074B80"  href="/2019/01/07/Ky_thuat_feature_engineering.html">Bài 1 - Kĩ thuật feature engineering</a></li>
					
				</nav>
			</div>
			<div class="col-md-8 col-xs-12" style="z-index:1">
				<nav class="navbar navbar-inverse" style="background-color: #046897">
					<div class = "container-fluid">
						<div class = "navbar-header>
							<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
								<span class="icon-bar"></span>
								<span class="icon-bar"></span>
								<span class="icon-bar"></span>
							</button>
							<a class="navbar-brand" href="/">
								<span style="color:#FFF">Khoa học dữ liệu - Khanh's blog</span>
							</a>
						</div>
						<br>
						<br>
						<div class="collapse navbar-collapse navbar-right" id="myNavbar">
							<ul class="nav navbar-nav">
								<li><a href="/home"><span style="color: #fff"> Home</span></a></li>
								<li><a href="/about"><span style="color: #fff"> About</span></a></li>
								<!-- <li><a href="/da"><span style="color: #fff">Data Analytics</span></a></li> -->
								<!-- <li><a href="/cv"><span style="color: #fff">Computer Vision</span></a></li> -->
								<!-- <li><a href="/nlp"><span style="color: #fff">NLP</span></a></li> -->
								<!-- <li><a href="/code"><span style="color: #fff">Code</span></a></li> -->
								<li><a href="/book"><span style="color: #fff">Book</span></a></li>
							</ul>
						</div>
					</div>
				</nav>
				<div class="PageNavigation">
				</div>
				<h1 itemprop="name" class="post-title"></h1>
				<div id="bootstrap-overrides">
					<div>
<h2><p class="post-link" style="text-align: left; color: #204081; font-weight: bold">Bài 7 - Pytorch - Buổi 2 - Seq2seq model correct spelling</p></h2> 
<strong>19 Aug 2019 - phamdinhkhanh</strong>
</div>
<br/>
<div id="toc"></div>
<h1 id="1-giới-thiệu-chung">1. Giới thiệu chung</h1>
<p>Cùng với sự phát triển của deep learning nói chung. Ngày nay lớp các mô hình seq2seq càng tỏ ra hiệu quả trong nhiều tác vụ khác nhau như dịch máy, sửa lỗi chính tả, image captioning, recommendation, dự báo chuỗi thời gian,…. Nhờ sự phát triển của các kiến trúc mạng RNN hiện đại kèm theo các kĩ thuật learning hiệu quả như sử dụng thêm kiến trúc <code class="highlighter-rouge">attention layer</code>, các phương pháp cải thiện accuracy như ` teach_forcing, beam search` mà mô hình dịch máy ngày càng đạt độ chính xác cao. Các tài liệu về các phương pháp trên đã có nhiều tuy nhiên đa phần là lý thuyết và khá khó cho người mới bắt đầu tiếp cận. Bài viết này nhằm mục đích tạo ra một bản diễn giải kèm thực hành về các bước xây dựng mô hình seq2seq ứng dụng trong sửa lỗi chính tả. Để hiểu được các nội dung trong bài yêu cầu bạn đọc có kiến thức nền tảng về <a href="https://phamdinhkhanh.github.io/2019/08/10/PytorchTurtorial1.html">pytorch</a>, nắm vững lý thuyết về mạng <a href="https://phamdinhkhanh.github.io/2019/04/22/L%C3%BD_thuy%E1%BA%BFt_v%E1%BB%81_m%E1%BA%A1ng_LSTM.html">LSTM</a>. Ngoài ra bạn đọc cần có sẵn máy tính cài <a href="https://pytorch.org/">pytorch</a> hoặc các VM hỗ trợ pytorch. Để tiện cho thực hành tôi khuyến nghị bạn đọc sử dụng <a href="https://colab.research.google.com">google colab</a> miễn phí và cài sẵn các deep learning framework cơ bản như tensorflow, pytorch, keras,….</p>

<p>Ngoài ra bài viết được xây dựng và tổng hợp dựa trên nhiều nguồn tài liệu khác nhau. Đặc biệt là từ trang hướng dẫn thực hành <a href="https://pytorch.org/tutorials/beginner/chatbot_tutorial.html">pytorch</a>, từ ý tưởng được chia sẻ nằm trong top của cuộc thi <a href="https://forum.machinelearningcoban.com/t/aivivn-3-vietnamese-tone-prediction-1st-place-solution/5721">thêm dấu cho tiếng việt - aivivn 1st</a>, <a href="https://forum.machinelearningcoban.com/t/aivivn-3-vietnamese-tone-prediction-2nd-place-solution/5759">thêm dấu cho tiếng việt - aivivn 2nd</a>.</p>

<p>Để huấn luyện mô hình trên google colab chúng ta cần mount folder lưu trữ dữ liệu trên google drive để có thể access dữ liệu dễ dàng. Bên dưới là câu lệnh thực hiện mount dữ liệu.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s">'/content/gdrive'</span><span class="p">)</span>
<span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s">'gdrive/My Drive/your_data_folder_link'</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
<span class="err">!</span><span class="n">ls</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Trong đó <code class="highlighter-rouge">your_data_folder_link</code> là đường link tới folder chứa dữ liệu. Lưu ý link <code class="highlighter-rouge">gdrive/My Drive</code> là đường trỏ mặc định để đi vào thư mục <code class="highlighter-rouge">My Drive</code> của bạn.</p>

<h1 id="2-chuẩn-bị-dữ-liệu">2. Chuẩn bị dữ liệu</h1>

<h2 id="21-giải-nén-dữ-liệu">2.1. Giải nén dữ liệu</h2>
<p>Ở bước này chúng ta sẽ sử dụng đầu vào là dữ liệu của cuộc thi thêm dấu từ tiếng việt tại <a href="https://www.aivivn.com/contests/3">aivivn</a>. Bạn đọc có thể tải về bộ dữ liệu tại <a href="https://drive.google.com/file/d/1m_5CDQQSavev5zWb8JUq97_zUTnOcVvS/view">google drive</a> dưới dạng zip file. Để giải nén dữ liệu ta cần extract bằng hàm extract bên dưới. Nhớ cài đặt package zipfile trước khi chạy lệnh.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="c1"># Extract zip file
</span><span class="kn">import</span> <span class="nn">zipfile</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="k">def</span> <span class="nf">_extract_zip_file</span><span class="p">(</span><span class="n">fn_zip</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">zipfile</span><span class="o">.</span><span class="n">ZipFile</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">zip_ref</span><span class="p">:</span>
      <span class="n">zip_ref</span><span class="o">.</span><span class="n">extractall</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

<span class="n">_extract_zip_file</span><span class="p">(</span><span class="n">fn_zip</span> <span class="o">=</span> <span class="s">'vietnamese_tone_prediction.zip'</span><span class="p">,</span> <span class="n">fn</span> <span class="o">=</span> <span class="s">'vietnamse_tone_prediction'</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s">'vietnamse_tone_prediction'</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Cùng lấy ra 500000 dòng đầu tiên của file <code class="highlighter-rouge">train.txt</code> trong folder giải nén làm tập huấn luyện.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">_data_train</span><span class="p">(</span><span class="n">fn</span><span class="p">):</span>
  <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span> <span class="k">as</span> <span class="n">fn</span><span class="p">:</span>
    <span class="n">train</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
  <span class="n">train</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">train</span><span class="p">[:</span><span class="mi">500000</span><span class="p">]]</span>  
  <span class="k">return</span> <span class="n">train</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">_data_train</span><span class="p">(</span><span class="n">fn</span> <span class="o">=</span> <span class="s">'vietnamse_tone_prediction/train.txt'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'length of train: {}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train</span><span class="p">)))</span>  
<span class="n">train</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre>length of train: 500000





['Bộ phim lần đầu được công chiếu tại liên hoan phim Rome 2007 và sau đó được chiếu ở Fairbanks, Alaska ngày 21 tháng 9 năm 2007.',
 'Những kiểu áo sơ mi may theo chất liệu cotton, KT, hay có chút co giãn năm nay cũng được các bạn trẻ ưa chuộng.',
 'Đương kim tổng thống là Andrés Manuel López Obrador, người nhậm chức vào ngày 1 tháng 12 năm 2018.',
 'Centaurea gloriosa là một loài thực vật có hoa trong họ Cúc.',
 'Sau này mới thấy người ta nói, đó là con rắn đực đi tìm người ăn thịt để trả thù cho rắn cái, bà T cho biết thêm.']
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Bên dưới là một số hàm tiện ích để chuyển các từ có dấu của tiếng việt sang không dấu. Mục đích là để tạo ra các cặp (câu không dấu, câu có dấu) từ câu có dấu.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="rouge-code"><pre><span class="c1"># encoding=utf8
</span><span class="kn">import</span> <span class="nn">codecs</span>
<span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="c1"># sys.setdefaultencoding('utf8')
</span>
<span class="k">def</span> <span class="nf">remove_tone_line</span><span class="p">(</span><span class="n">utf8_str</span><span class="p">):</span>
    <span class="n">intab_l</span> <span class="o">=</span> <span class="s">"ạảãàáâậầấẩẫăắằặẳẵóòọõỏôộổỗồốơờớợởỡéèẻẹẽêếềệểễúùụủũưựữửừứíìịỉĩýỳỷỵỹđ"</span>
    <span class="n">intab_u</span> <span class="o">=</span> <span class="s">"ẠẢÃÀÁÂẬẦẤẨẪĂẮẰẶẲẴÓÒỌÕỎÔỘỔỖỒỐƠỜỚỢỞỠÉÈẺẸẼÊẾỀỆỂỄÚÙỤỦŨƯỰỮỬỪỨÍÌỊỈĨÝỲỶỴỸĐ"</span>
    <span class="n">intab</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">intab_l</span><span class="o">+</span><span class="n">intab_u</span><span class="p">)</span>

    <span class="n">outtab_l</span> <span class="o">=</span> <span class="s">"a"</span><span class="o">*</span><span class="mi">17</span> <span class="o">+</span> <span class="s">"o"</span><span class="o">*</span><span class="mi">17</span> <span class="o">+</span> <span class="s">"e"</span><span class="o">*</span><span class="mi">11</span> <span class="o">+</span> <span class="s">"u"</span><span class="o">*</span><span class="mi">11</span> <span class="o">+</span> <span class="s">"i"</span><span class="o">*</span><span class="mi">5</span> <span class="o">+</span> <span class="s">"y"</span><span class="o">*</span><span class="mi">5</span> <span class="o">+</span> <span class="s">"d"</span>
    <span class="n">outtab_u</span> <span class="o">=</span> <span class="s">"A"</span><span class="o">*</span><span class="mi">17</span> <span class="o">+</span> <span class="s">"O"</span><span class="o">*</span><span class="mi">17</span> <span class="o">+</span> <span class="s">"E"</span><span class="o">*</span><span class="mi">11</span> <span class="o">+</span> <span class="s">"U"</span><span class="o">*</span><span class="mi">11</span> <span class="o">+</span> <span class="s">"I"</span><span class="o">*</span><span class="mi">5</span> <span class="o">+</span> <span class="s">"Y"</span><span class="o">*</span><span class="mi">5</span> <span class="o">+</span> <span class="s">"D"</span>
    <span class="n">outtab</span> <span class="o">=</span> <span class="n">outtab_l</span> <span class="o">+</span> <span class="n">outtab_u</span>

    <span class="n">r</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="s">"|"</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">intab</span><span class="p">))</span>
    <span class="n">replaces_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">intab</span><span class="p">,</span> <span class="n">outtab</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">r</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="n">replaces_dict</span><span class="p">[</span><span class="n">m</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">0</span><span class="p">)],</span> <span class="n">utf8_str</span><span class="p">)</span>
  
<span class="n">remove_tone_line</span><span class="p">(</span><span class="s">'Đi một ngày đàng học 1 sàng khôn'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>'Di mot ngay dang hoc 1 sang khon'
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Để ý thấy hầu hết các dấu câu sẽ dính liền với câu. Chẳng hạn câu <code class="highlighter-rouge">tại khanh blog, tôi lưu lại các bài viết như một quyển nhật kí</code>. Thì từ <code class="highlighter-rouge">blog</code> và dấu phảy bị dính liền. Vì vậy ta cần sử dụng hàm <code class="highlighter-rouge">normalizeString()</code> để tách các dấu ra khỏi từ.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="c1"># Tách dấu ra khỏi từ
</span><span class="k">def</span> <span class="nf">normalizeString</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="c1"># Tách dấu câu nếu kí tự liền nhau
</span>    <span class="n">marks</span> <span class="o">=</span> <span class="s">'[.!?,-${}()]'</span>
    <span class="n">r</span> <span class="o">=</span> <span class="s">"(["</span><span class="o">+</span><span class="s">"</span><span class="se">\\</span><span class="s">"</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">marks</span><span class="p">)</span><span class="o">+</span><span class="s">"])"</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="s">r" \1 "</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="c1"># Thay thế nhiều spaces bằng 1 space.
</span>    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">r"\s+"</span><span class="p">,</span> <span class="s">r" "</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">s</span>

<span class="n">normalizeString</span><span class="p">(</span><span class="s">'vui vẻ, hòa đồng, hoạt bát'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>'vui vẻ , hòa đồng , hoạt bát'
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Sử dụng 2 hàm số trên để tạo ra các list <code class="highlighter-rouge">train</code> gồm các câu có dấu và <code class="highlighter-rouge">train_rev_accent</code> gồm các câu không dấu.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">itertools</span>
<span class="n">train</span> <span class="o">=</span> <span class="p">[</span><span class="n">normalizeString</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">train</span><span class="p">]</span>
<span class="n">train_rev_accent</span> <span class="o">=</span> <span class="p">[</span><span class="n">remove_tone_line</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">train</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s">'train top 5:'</span><span class="p">,</span> <span class="n">train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">'train_rev_accent top 5:'</span><span class="p">,</span> <span class="n">train_rev_accent</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>train top 5: ['Bộ phim lần đầu được công chiếu tại liên hoan phim Rome 2007 và sau đó được chiếu ở Fairbanks , Alaska ngày 21 tháng 9 năm 2007 .', 'Những kiểu áo sơ mi may theo chất liệu cotton , KT , hay có chút co giãn năm nay cũng được các bạn trẻ ưa chuộng .', 'Đương kim tổng thống là Andrés Manuel López Obrador , người nhậm chức vào ngày 1 tháng 12 năm 2018 .', 'Centaurea gloriosa là một loài thực vật có hoa trong họ Cúc .', 'Sau này mới thấy người ta nói , đó là con rắn đực đi tìm người ăn thịt để trả thù cho rắn cái , bà T cho biết thêm .']
train_rev_accent top 5: ['Bo phim lan dau duoc cong chieu tai lien hoan phim Rome 2007 va sau do duoc chieu o Fairbanks , Alaska ngay 21 thang 9 nam 2007 .', 'Nhung kieu ao so mi may theo chat lieu cotton , KT , hay co chut co gian nam nay cung duoc cac ban tre ua chuong .', 'Duong kim tong thong la Andres Manuel Lopez Obrador , nguoi nham chuc vao ngay 1 thang 12 nam 2018 .', 'Centaurea gloriosa la mot loai thuc vat co hoa trong ho Cuc .', 'Sau nay moi thay nguoi ta noi , do la con ran duc di tim nguoi an thit de tra thu cho ran cai , ba T cho biet them .']
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="12-load-and-trim-data">1.2. Load and Trim data</h2>

<p>Bên dưới ta sẽ xây dựng class Voc để xây dựng từ điển cho tập dữ liệu. Các hàm trong Voc có tác dụng:</p>

<ul>
  <li><code class="highlighter-rouge">addWord()</code>: Kiểm tra xem từ đã xuất hiện trong từ điển chưa. Nếu chưa sẽ thêm từ mới đó vào từ điển.</li>
  <li><code class="highlighter-rouge">addSentence()</code>: Truyền vào 1 câu và thêm các từ trong câu vào từ điển.</li>
  <li><code class="highlighter-rouge">trim()</code>: Loại bỏ các từ hiếm trong từ điển nếu nó có ngưỡng số tần xuất xuất hiện trong toàn bộ tập dữ liệu ít hơn <code class="highlighter-rouge">min_count</code>.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
</pre></td><td class="rouge-code"><pre><span class="c1"># Default word tokens
</span><span class="n">PAD_token</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Used for padding short sentences
</span><span class="n">SOS_token</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Start-of-sentence token
</span><span class="n">EOS_token</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># End-of-sentence token
</span>

<span class="k">class</span> <span class="nc">Voc</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trimmed</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD_token</span><span class="p">:</span> <span class="s">"PAD"</span><span class="p">,</span> <span class="n">SOS_token</span><span class="p">:</span> <span class="s">"SOS"</span><span class="p">,</span> <span class="n">EOS_token</span><span class="p">:</span> <span class="s">"EOS"</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Count SOS, EOS, PAD
</span>
    <span class="k">def</span> <span class="nf">addSentence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">' '</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">addWord</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">addWord</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_words</span><span class="p">]</span> <span class="o">=</span> <span class="n">word</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Remove words below a certain count threshold
</span>    <span class="k">def</span> <span class="nf">trim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_count</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trimmed</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trimmed</span> <span class="o">=</span> <span class="bp">True</span>

        <span class="n">keep_words</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">v</span> <span class="o">&gt;=</span> <span class="n">min_count</span><span class="p">:</span>
                <span class="n">keep_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="s">'keep_words {} / {} = {:.4f}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="n">keep_words</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_words</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word2index</span><span class="p">)</span>
        <span class="p">))</span>

        <span class="c1"># Reinitialize dictionaries
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">word2index</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2count</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">PAD_token</span><span class="p">:</span> <span class="s">"PAD"</span><span class="p">,</span> <span class="n">SOS_token</span><span class="p">:</span> <span class="s">"SOS"</span><span class="p">,</span> <span class="n">EOS_token</span><span class="p">:</span> <span class="s">"EOS"</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_words</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># Count default tokens
</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">keep_words</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">addWord</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="12-chuẩn-hóa-dữ-liệu">1.2. Chuẩn hóa dữ liệu</h2>

<h3 id="121-tạo-ngram">1.2.1 Tạo ngram</h3>

<p>Trong dữ liệu có những câu rất dài làm giảm hiệu quả dự báo của mô hình. Do đó chúng ta sẽ tìm cách tạo ra các ngram với độ dài đo bằng số lượng từ cố định để dự báo trong một khoảng cách ngắn. Ở bài này ta sẽ lựa chọn ngram = 4.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">_ngram</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">length</span> <span class="o">=</span> <span class="mi">4</span><span class="p">):</span>
  <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
  <span class="n">grams</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">length</span><span class="p">:</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">words</span> <span class="o">+</span> <span class="p">[</span><span class="s">"PAD"</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">length</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">[</span><span class="s">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)]</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span><span class="o">-</span><span class="n">length</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
      <span class="n">grams</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="n">length</span><span class="p">)]))</span>
    <span class="k">return</span> <span class="n">grams</span>
  
<span class="k">print</span><span class="p">(</span><span class="n">_ngram</span><span class="p">(</span><span class="s">'mùa đông năm nay không còn lạnh nữa. Vì đã có gấu 37 độ ấm'</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">_ngram</span><span class="p">(</span><span class="s">'mùa đông'</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>['mùa đông năm nay', 'đông năm nay không', 'năm nay không còn', 'nay không còn lạnh', 'không còn lạnh nữa.', 'còn lạnh nữa. Vì', 'lạnh nữa. Vì đã', 'nữa. Vì đã có', 'Vì đã có gấu', 'đã có gấu 37', 'có gấu 37 độ', 'gấu 37 độ ấm']
['mùa đông PAD PAD']
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">itertools</span>

<span class="n">train_grams</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">([</span><span class="n">_ngram</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">train</span><span class="p">]))</span>
<span class="n">train_rev_acc_grams</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">([</span><span class="n">_ngram</span><span class="p">(</span><span class="n">item</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">train_rev_accent</span><span class="p">]))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="n">corpus</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">train_rev_acc_grams</span><span class="p">,</span> <span class="n">train_grams</span><span class="p">))</span>
<span class="n">corpus</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre>[('Bo phim lan dau', 'Bộ phim lần đầu'),
 ('phim lan dau duoc', 'phim lần đầu được'),
 ('lan dau duoc cong', 'lần đầu được công'),
 ('dau duoc cong chieu', 'đầu được công chiếu'),
 ('duoc cong chieu tai', 'được công chiếu tại')]
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">unicodedata</span>

<span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># Maximum sentence length to consider
</span>
<span class="c1"># Turn a Unicode string to plain ASCII, thanks to
# https://stackoverflow.com/a/518232/2809427
</span><span class="k">def</span> <span class="nf">unicodeToAscii</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="s">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
        <span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="s">'NFD'</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">unicodedata</span><span class="o">.</span><span class="n">category</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">!=</span> <span class="s">'Mn'</span>
    <span class="p">)</span>

<span class="c1"># Lowercase, trim, and remove non-letter characters
</span><span class="k">def</span> <span class="nf">normalizeString</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="c1"># Tách dấu câu nếu kí tự liền nhau
</span>    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">r"([.!?,\-\&amp;\(\)\[\]])"</span><span class="p">,</span> <span class="s">r" \1 "</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
    <span class="c1"># Thay thế nhiều spaces bằng 1 space.
</span>    <span class="n">s</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">r"\s+"</span><span class="p">,</span> <span class="s">r" "</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">s</span>

<span class="c1"># Read query/response pairs and return a voc object
</span><span class="k">def</span> <span class="nf">readVocs</span><span class="p">(</span><span class="n">lines</span><span class="p">,</span> <span class="n">corpus_name</span> <span class="o">=</span> <span class="s">'corpus'</span><span class="p">):</span>
    <span class="c1"># Split every line into pairs and normalize
</span>    <span class="n">pairs</span> <span class="o">=</span> <span class="p">[[</span><span class="n">normalizeString</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">s</span><span class="p">))</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">l</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">]</span>
    <span class="n">voc</span> <span class="o">=</span> <span class="n">Voc</span><span class="p">(</span><span class="n">corpus_name</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span>

<span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">readVocs</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>  
  
<span class="c1"># Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold
</span><span class="k">def</span> <span class="nf">filterPair</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="c1"># Input sequences need to preserve the last word for EOS token
</span>    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">' '</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">MAX_LENGTH</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">' '</span><span class="p">))</span> <span class="o">&lt;</span> <span class="n">MAX_LENGTH</span>

<span class="c1"># Filter pairs using filterPair condition
</span><span class="k">def</span> <span class="nf">filterPairs</span><span class="p">(</span><span class="n">pairs</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">pair</span> <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span> <span class="k">if</span> <span class="n">filterPair</span><span class="p">(</span><span class="n">pair</span><span class="p">)]</span>

<span class="c1"># # Using the functions defined above, return a populated voc object and pairs list
</span><span class="k">def</span> <span class="nf">loadPrepareData</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Read {!s} sentence pairs"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)))</span>
    <span class="c1"># pairs = filterPairs(pairs)
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"Trimmed to {!s} sentence pairs"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Counting words..."</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
        <span class="n">voc</span><span class="o">.</span><span class="n">addSentence</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">voc</span><span class="o">.</span><span class="n">addSentence</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Counted words:"</span><span class="p">,</span> <span class="n">voc</span><span class="o">.</span><span class="n">num_words</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span>
  
<span class="c1"># Load/Assemble voc and pairs
</span><span class="n">save_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s">"data"</span><span class="p">,</span> <span class="s">"save"</span><span class="p">)</span>
<span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span> <span class="o">=</span> <span class="n">loadPrepareData</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">)</span>
<span class="c1"># Print some pairs to validate
</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">pairs:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">[:</span><span class="mi">10</span><span class="p">]:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre>Read 9771412 sentence pairs
Trimmed to 9771412 sentence pairs
Counting words...
Counted words: 202153

pairs:
['Bo phim lan dau', 'Bộ phim lần đầu']
['phim lan dau duoc', 'phim lần đầu được']
['lan dau duoc cong', 'lần đầu được công']
['dau duoc cong chieu', 'đầu được công chiếu']
['duoc cong chieu tai', 'được công chiếu tại']
['cong chieu tai lien', 'công chiếu tại liên']
['chieu tai lien hoan', 'chiếu tại liên hoan']
['tai lien hoan phim', 'tại liên hoan phim']
['lien hoan phim Rome', 'liên hoan phim Rome']
['hoan phim Rome 2007', 'hoan phim Rome 2007']
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Model sẽ huấn luyện nhanh hơn nếu:</p>

<ul>
  <li>Giảm bớt số lượng các token không quá phổ biến.</li>
  <li>Loại bỏ bớt các câu không phổ biến.</li>
</ul>

<p>Bên dưới ta sẽ xây dựng hàm số loại bỏ các token có tần suất nhỏ hơn ngưỡng MIN_COUNT và loại bỏ các câu có chứa token vừa bị loại bỏ.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
</pre></td><td class="rouge-code"><pre><span class="n">MIN_COUNT</span> <span class="o">=</span> <span class="mi">3</span>    <span class="c1"># Minimum word count threshold for trimming
</span>
<span class="k">def</span> <span class="nf">trimRareWords</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">,</span> <span class="n">MIN_COUNT</span><span class="p">):</span>
    <span class="c1"># Trim words used under the MIN_COUNT from the voc
</span>    <span class="n">voc</span><span class="o">.</span><span class="n">trim</span><span class="p">(</span><span class="n">MIN_COUNT</span><span class="p">)</span>
    <span class="c1"># Filter out pairs with trimmed words
</span>    <span class="n">keep_pairs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
        <span class="n">input_sentence</span> <span class="o">=</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">output_sentence</span> <span class="o">=</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">keep_input</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="n">keep_output</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="c1"># Check input sentence
</span>        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">input_sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">' '</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">voc</span><span class="o">.</span><span class="n">word2index</span><span class="p">:</span>
                <span class="n">keep_input</span> <span class="o">=</span> <span class="bp">False</span>
                <span class="k">break</span>
        <span class="c1"># Check output sentence
</span>        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">output_sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">' '</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">voc</span><span class="o">.</span><span class="n">word2index</span><span class="p">:</span>
                <span class="n">keep_output</span> <span class="o">=</span> <span class="bp">False</span>
                <span class="k">break</span>

        <span class="c1"># Only keep pairs that do not contain trimmed word(s) in their input or output sentence
</span>        <span class="k">if</span> <span class="n">keep_input</span> <span class="ow">and</span> <span class="n">keep_output</span><span class="p">:</span>
            <span class="n">keep_pairs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pair</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"Trimmed from {} pairs to {}, {:.4f} of total"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_pairs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">keep_pairs</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">pairs</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">keep_pairs</span>


<span class="c1"># Trim voc and pairs
</span><span class="n">pairs</span> <span class="o">=</span> <span class="n">trimRareWords</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">,</span> <span class="n">MIN_COUNT</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>keep_words 171772 / 202150 = 0.8497
Trimmed from 9771412 pairs to 9741582, 0.9969 of total
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Như vậy sau khi loại bỏ các từ hiếm với tần xuất xuất hiện &lt;= 3 thì dữ liệu còn lại 87% số lượng các token và 99.81% các câu được dữ lại.</p>

<h2 id="13-tạo-batch-huấn-luyện">1.3. Tạo batch huấn luyện</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="k">print</span><span class="p">(</span><span class="s">'EOS_token: '</span><span class="p">,</span> <span class="n">EOS_token</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'SOS_token: '</span><span class="p">,</span> <span class="n">SOS_token</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'PAD_token: '</span><span class="p">,</span> <span class="n">PAD_token</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre>EOS_token:  2
SOS_token:  1
PAD_token:  0
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Bên dưới ta sẽ xây dựng các hàm chức năng, trong đó:</p>

<ul>
  <li>
    <p><code class="highlighter-rouge">indexesFromSentence()</code>: Mã hóa câu văn thành chuỗi index của các token theo giá trị của cặp word2index trong từ điển và đính thêm token EOS ở cuối để đánh dấu kết thúc câu. Chẳng hạn trong từ điển từ các từ tương ứng với index như sau: {‘học’:5, ‘sinh’:7, ‘đi’:9, ‘EOS’:2}, khi đó giá trị mã hóa index của câu ‘học sinh đi học’ sẽ là [5, 7, 9, 5,2].</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">zeroPadding()</code>: Nhận đầu vào là 1 list các chuỗi index đại diện cho câu. Hàm này sẽ xác định câu có độ dài lớn nhất trong list. Sau đó padding thêm 0 vào cuối mỗi chuỗi index các giá trị 0 về cuối để các câu có độ dài bằng nhau và bằng độ dài lớn nhất.</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">binaryMatrix()</code>: Một ma trận sẽ biểu diễn một batch của các câu truyền vào. Mỗi dòng của ma trận sẽ đại diện cho 1 câu. Trong ma trận này sẽ tồn tại những index tương ứng với vị trí padding. binaryMatrix sẽ đánh dấu các vị trí mà tương ứng với padding bằng 0 và tương ứng với từ bằng 1.</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">inputVar()</code>: Nhận giá trị truyền vào là 1 list các câu input và từ điển. Hàm sẽ trả về ma trận được padding thêm 0 đại diện cho list các câu input và list độ dài tương ứng thực tế của các câu.</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">outputVar()</code>: Nhận giá trị truyền vào là 1 list các câu output và từ điển. Về cơ bản cũng giống như <code class="highlighter-rouge">inputVar()</code> nhưng ngoài trả về ma trận padding và độ dài lớn nhất của các câu còn trả về thêm ma trận mask có kích thước bằng ma trận padding đánh dấu các vị trí là padding (=0) và từ (=1).</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">batch2TrainData()</code>: Nhận giá trị truyền vào là các cặp câu (input, output) và từ điển. Hàm số sẽ khởi tạo batch cho huấn luyện mô hình bao gồm: ma trận batch input, ma trận batch output, ma trận mask đánh đấu padding của output. Ngoài ra còn trả thêm list độ dài thực tế các câu trong input và độ dài lớn nhất của các câu trong output.</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">voc</span><span class="o">.</span><span class="n">word2index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">' '</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">EOS_token</span><span class="p">]</span>

<span class="c1"># Padding thêm 0 vào list nào có độ dài nhỏ hơn về phía bên phải
</span><span class="k">def</span> <span class="nf">zeroPadding</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">fillvalue</span><span class="o">=</span><span class="n">PAD_token</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">zip_longest</span><span class="p">(</span><span class="o">*</span><span class="n">l</span><span class="p">,</span> <span class="n">fillvalue</span><span class="o">=</span><span class="n">fillvalue</span><span class="p">))</span>
  
<span class="c1"># Tạo ma trận binary có kích thước như ma trận truyền vào l nhưng giá trị của mỗi phần tử đánh dấu 1 hoặc 0 tương ứng với padding hoặc không padding
</span><span class="k">def</span> <span class="nf">binaryMatrix</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">PAD_token</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">seq</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">l</span><span class="p">):</span>
        <span class="n">m</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="n">PAD_token</span><span class="p">:</span>
                <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span>

<span class="c1"># Returns padded input sequence tensor and lengths
</span><span class="k">def</span> <span class="nf">inputVar</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">voc</span><span class="p">):</span>
    <span class="n">indexes_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">l</span><span class="p">]</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span> <span class="k">for</span> <span class="n">indexes</span> <span class="ow">in</span> <span class="n">indexes_batch</span><span class="p">])</span>
    <span class="n">padList</span> <span class="o">=</span> <span class="n">zeroPadding</span><span class="p">(</span><span class="n">indexes_batch</span><span class="p">)</span>
    <span class="n">padVar</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">padList</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">padVar</span><span class="p">,</span> <span class="n">lengths</span>

<span class="c1"># Returns padded target sequence tensor, padding mask, and max target length
</span><span class="k">def</span> <span class="nf">outputVar</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">voc</span><span class="p">):</span>
    <span class="n">indexes_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">l</span><span class="p">]</span>
    <span class="n">max_target_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span> <span class="k">for</span> <span class="n">indexes</span> <span class="ow">in</span> <span class="n">indexes_batch</span><span class="p">])</span>
    <span class="n">padList</span> <span class="o">=</span> <span class="n">zeroPadding</span><span class="p">(</span><span class="n">indexes_batch</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">binaryMatrix</span><span class="p">(</span><span class="n">padList</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ByteTensor</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
    <span class="n">padVar</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">padList</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">padVar</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span>

<span class="c1"># Returns all items for a given batch of pairs
</span><span class="k">def</span> <span class="nf">batch2TrainData</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">pair_batch</span><span class="p">):</span>
    <span class="n">pair_batch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">" "</span><span class="p">)),</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">input_batch</span><span class="p">,</span> <span class="n">output_batch</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pair_batch</span><span class="p">:</span>
        <span class="n">input_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">output_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">inp</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="n">inputVar</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">voc</span><span class="p">)</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span> <span class="o">=</span> <span class="n">outputVar</span><span class="p">(</span><span class="n">output_batch</span><span class="p">,</span> <span class="n">voc</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inp</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span>


<span class="c1"># Example for validation
</span><span class="n">small_batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">batch2TrainData</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">small_batch_size</span><span class="p">)])</span>
<span class="n">input_variable</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span> <span class="o">=</span> <span class="n">batches</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Cuối cùng ta sẽ thử nghiệm giá trị trả ra của hàm <code class="highlighter-rouge">batch2TrainData()</code> khi lựa chọn ra 4 cặp câu bất kì trong list các cặp (input, output).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="k">print</span><span class="p">(</span><span class="s">"input_variable: </span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">input_variable</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"lengths: </span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"target_variable: </span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"mask: </span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"max_target_len: </span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">max_target_len</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="rouge-code"><pre>input_variable: 
 tensor([[  66,  369,   66, 1272],
        [ 567,  183,   28,  616],
        [ 392, 1558, 1143,  175],
        [ 394,   31,   31, 5558],
        [   2,    2,    2,    2]])
lengths: 
 tensor([5, 5, 5, 5])
target_variable: 
 tensor([[  66,  370,  124, 1275],
        [ 568,  184,   29,  617],
        [ 393, 1560, 1144, 6240],
        [ 395,   31,   31, 5559],
        [   2,    2,    2,    2]])
mask: 
 tensor([[1, 1, 1, 1],
        [1, 1, 1, 1],
        [1, 1, 1, 1],
        [1, 1, 1, 1],
        [1, 1, 1, 1]], dtype=torch.uint8)
max_target_len: 
 5
</pre></td></tr></tbody></table></code></pre></div></div>

<h1 id="2-mô-hình-seq2seq-sửa-lỗi-chính-tả">2. Mô hình seq2seq sửa lỗi chính tả</h1>

<p>Model seq2seq sẽ nhận đầu vào là 1 chuỗi và trả ra kết quả output cũng là 1 chuỗi. Chính vì thế tên gọi của mô hình là sequence to sequence (từ câu đến câu).</p>

<p>Trong kiến trúc của model seq2seq sẽ gồm 2 phrases: Encoder và decoder.</p>

<ul>
  <li>
    <p><strong>Encoder</strong>: Nhúng các từ thành những véc tơ embedding với kích thước tùy ý. Encoder sẽ xây dựng một chuỗi các xử lý liên hoàn sao cho output của bước liền trước là input của bước liền sau. Khi đó tại mỗi time step sẽ truyền đầu vào là các véc tơ đã được mã hóa ứng với mỗi từ $\mathbf{x_i}$. Encoder sẽ  trả ra 2 kết quả ở đầu ra gồm: 
encoder outputs đại diện cho toàn bộ câu input trong đó mỗi véc tơ của encoder outputs đại diện cho 1 từ trong câu và hidden state của GRU cuối cùng.  hidden state sẽ được sử dụng để làm giá trị hidden khởi tạo cho quá trình Decoder (chi tiết ở hình 1 bên dưới). ma trận encoder outputs được sử dụng để tính attention weight tại mỗi time step trong phrase decoder. Để dễ hình dung output của một mạng RNN chúng ta có thể xem thêm <a href="https://phamdinhkhanh.github.io/2019/04/22/L%C3%BD_thuy%E1%BA%BFt_v%E1%BB%81_m%E1%BA%A1ng_LSTM.html">lý thuyết về mạng LSTM</a>.</p>
  </li>
  <li>
    <p>Decoder: Sau phrase encoder ta sẽ thu được một hidden state của GRU cuối cùng và ma trận encoder outputs đại điện cho toàn bộ câu input. Phrase decoder có tác dụng giải mã thông tin đầu ra ở encoder thành các từ. Do đó tại mỗi time step đều trả ra các véc tơ phân phối xác xuất của từ tại bước đó. Từ đó ta có thể xác định được từ có khả năng xảy ra nhất tại mỗi time step. Tại time step $t$, mô hình sẽ  kết hợp giữa decoder embedding véc tơ $h_t$ đại diện cho token $word_t$ và ma trận encoder outputs theo cơ chế global attention (được đề xuất bởi anh Lương Mạnh Thắng) để tính ra trọng số attention weight phân bố cho vị trí từ ở câu input lên $word_t$. véc tơ context đại diện cho toàn bộ câu input sẽ được tính bằng tích trọng số của attention weight với từng encoder véc tơ của ma trận encoder outputs (cụ thể hình 3). Tiếp theo để dự báo cho từ kế tiếp $word_{t+1}$ ta cần kết hợp véc tơ decoder hidden state $h_{t+1}$ và véc tơ context như hình 5. Qúa trình này sẽ được lặp lại liên tục cho đến khi gặp token cuối cùng là <code class="highlighter-rouge">&lt;EOS&gt;</code> đánh dấu vị trí cuối của câu. Như vậy ta sẽ trải qua các bước:</p>

    <ul>
      <li>
        <p>Tính decoder input chính là embedding véc tơ $h_t$ của từ đã biết $word_t$ dựa vào embedding layer.</p>
      </li>
      <li>
        <p>Tính attention weight đánh giá mức độ tập trung của các từ input vào từ được dự báo dựa vào ma trận encoder outputs và $h_{t}$.</p>
      </li>
      <li>
        <p>Tính context véc tơ $c_t$ chính là tích có trọng số của attention weights với các véc tơ đại diện cho mỗi từ input trong ma trận encoder outputs.</p>
      </li>
      <li>
        <p>Truyền decoder input và last hidden state ở bước $t$ vào mô hình decoder GRU để tính ra được decoder hidden state $h_{t+1}$.</p>
      </li>
      <li>
        <p>Kết hợp decoder hidden state $h_{t+1}$ và context véc tơ $c_t$ để dự báo từ tại time step $t+1$.</p>
      </li>
    </ul>
  </li>
</ul>

<p>Quá trình tiếp tục cho đến khi gặp token <code class="highlighter-rouge">&lt;EOS&gt;</code> đánh dấu kết thúc câu.</p>

<p>Kết quả đầu ra là chuỗi các indexes (lấy theo vocabulary) đại diện cho từng từ tại mỗi vị trí của câu.</p>

<p><img src="https://pytorch.org/tutorials/_images/seq2seq_ts.png" alt="Seq2seq model" /></p>
<blockquote>
  <p><strong>Hình 1:</strong> Sơ đồ Encoder và Decoder sử dụng mạng GRU trong mô hình seq2seq.</p>
</blockquote>

<h2 id="21-encoder">2.1. Encoder</h2>

<p>Tại phrase encoder chúng ta sẽ sử dụng kiến trúc mạng bidirectional GRU (2 chiều) như bên dưới để mã hóa thông tin.</p>

<p><img src="https://pytorch.org/tutorials/_images/RNN-bidirectional.png" alt="Bidirectional GRU" /></p>
<blockquote>
  <p><strong>Hình 2:</strong> Kiến trúc mạng bidirectional GRU. Khác với mạng unidirectional GRU (1 chiều) chỉ có chiều từ trái qua phải. Mạng GRU 2 chiều sẽ đánh giá thêm chiều từ phải qua trái. Điều này giúp cho việc học được đầy đủ hơn khi sự phụ thuộc của các từ trong câu luôn tuân theo cả 2 chiều.</p>
</blockquote>

<p>Lưu ý một embedding layer được áp dụng để mã hóa các từ về một véc tơ với kích thước được khai báo là <code class="highlighter-rouge">hidden_size</code>. Khi huấn luyện xong model, embedding layer sẽ có kết quả sao cho những từ gần nghĩa sẽ được đại diện bởi những vec tơ sao cho độ tương quan về nghĩa càng lớn. Độ tương quan này được đo bằng cosin similarity của các embedding véc tơ của mỗi từ.</p>

<p>Mô hình RNN sẽ nhận đầu vào là một batch. Để padding một batch vào model RNN thì chúng ta phải pack dữ liệu bằng hàm <code class="highlighter-rouge">nn.utils.rnn.pack_padded_sequence</code> để tự động padding thêm 0 vào các véc tơ từ. Và ở bước decoder chúng ta phải unpack phần zero padding bao quanh đầu ra bằng hàm <code class="highlighter-rouge">nn.utils.rnn.pad_packed_sequence</code>.</p>

<p>Bên dưới ta sẽ xây dựng Module encoder.</p>

<p><strong>Quá trình tính toán của đồ thị:</strong></p>
<ol>
  <li>Mã hóa các từ về index và embedding các index thành các véc tơ.</li>
  <li>Xác định batch cho mô hình. Padding câu về chung 1 độ dài và đóng gói thành batch bằng hàm <code class="highlighter-rouge">nn.utils.rnn.pack_padded_sequence</code>.</li>
  <li>Thực hiện quá trình feed forward.</li>
  <li>unpack các zero padding bằng hàm <code class="highlighter-rouge">nn.utils.rnn.pad_packed_sequence</code></li>
  <li>Tính tổng của bidirectional GRU outputs theo hai chiều trái và phải.</li>
  <li>Trả về encoder output của layer GRU cuối cùng và hidden state tại layer cuối cùng.</li>
</ol>

<p>Các tham số của model:</p>

<p><strong>Inputs</strong>:</p>

<ul>
  <li><code class="highlighter-rouge">input_seq</code>: Batch của các câu đầu vào dưới dạng tensor (như tham số input được trả ra của hàm <code class="highlighter-rouge">batch2trainData()</code>). <code class="highlighter-rouge">shape = max_length x batch_size</code>. Trong đó <code class="highlighter-rouge">batch_size</code> là kích thước batch (tương ứng với số câu được đưa vào batch) và <code class="highlighter-rouge">max_length</code> là kích thước của câu văn đã được pad hoặc trim về độ dài chuẩn.</li>
  <li><code class="highlighter-rouge">input_lengths</code>: list của độ dài thực tế (không tính pad hoặc trim) các câu tương ứng trong batch.</li>
</ul>

<p><strong>Outputs</strong>:</p>

<ul>
  <li>
    <p><code class="highlighter-rouge">outputs</code>: hidden layer cuối cùng của GRU (tổng của bidirectional outputs). Trên hình chính là GRU  tại vị trí $\mathbf{x_n}$; <code class="highlighter-rouge">shape = (max_length, batch_size, hidden_size)</code> . Trong đó <code class="highlighter-rouge">max_length</code> là độ dài của câu. <code class="highlighter-rouge">batch_size</code> là kích thước batch. <code class="highlighter-rouge">hidden_size</code> là số chiều mã hóa của embedding layer.</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">hidden</code>: là ma trận concatenate của các hidden state được cập nhật từ mỗi layer của GRU; <code class="highlighter-rouge">shape = (n_layers x num_directions, batch_size, hidden_size)</code>. Trong đó:</p>

    <ul>
      <li>
        <p><code class="highlighter-rouge">n_layers</code> là số layers GRU được áp dụng. Số lượng layers này sẽ bằng chính số lượng hidden state của mạng GRU.</p>
      </li>
      <li>
        <p><code class="highlighter-rouge">hidden_size</code> chính là kích thước của embedding véc tơ ứng với mỗi từ.</p>
      </li>
      <li>
        <p><code class="highlighter-rouge">num_bidirections</code> là số chiều của mạng GRU. Trong trường hợp này là 2 chiều.</p>
      </li>
    </ul>
  </li>
</ul>

<p>Trước tiên ta cần khai báo sử dụng pytorch cuda bằng đoạn code bên dưới. hàm <code class="highlighter-rouge">torch.cuda.is_available()</code> sẽ tự động kiểm tra xem máy có GPU hỗ trợ CUDA hay không. Nếu có sẽ khai báo <code class="highlighter-rouge">device</code> là <code class="highlighter-rouge">cuda</code> và trái lại sẽ sử dụng <code class="highlighter-rouge">cpu</code> để huấn luyện mô hình. Và tất nhiên là tốc độ huấn luyện trên <code class="highlighter-rouge">cuda</code> nhanh hơn rất nhiều.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">unicode_literals</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.jit</span> <span class="kn">import</span> <span class="n">script</span><span class="p">,</span> <span class="n">trace</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">unicodedata</span>
<span class="kn">import</span> <span class="nn">codecs</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="nb">open</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">math</span>


<span class="n">USE_CUDA</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">USE_CUDA</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">EncoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span>

        <span class="c1"># Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'
</span>        <span class="c1"># set bidirectional = True for bidirectional
</span>        <span class="c1"># https://pytorch.org/docs/stable/nn.html?highlight=gru#torch.nn.GRU to get more information
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="c1"># number of expected feature of input x 
</span>                          <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="c1"># number of expected feature of hidden state 
</span>                          <span class="n">num_layers</span> <span class="o">=</span> <span class="n">n_layers</span><span class="p">,</span> <span class="c1"># number of GRU layers
</span>                          <span class="n">dropout</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">n_layers</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">dropout</span><span class="p">),</span> <span class="c1"># dropout probability apply in encoder network
</span>                          <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span> <span class="c1"># one or two directions.
</span>                         <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">hidden</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Step 1: Convert word indexes to embeddings
</span>        <span class="c1"># shape: (max_length , batch_size , hidden_size)
</span>        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">input_seq</span><span class="p">)</span>
        <span class="c1"># Pack padded batch of sequences for RNN module. Padding zero when length less than max_length of input_lengths.
</span>        <span class="c1"># shape: (max_length , batch_size , hidden_size)
</span>        <span class="n">packed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pack_padded_sequence</span><span class="p">(</span><span class="n">embedded</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">)</span>
        <span class="c1"># Step 2: Forward packed through GRU
</span>        <span class="c1"># outputs is output of final GRU layer
</span>        <span class="c1"># hidden is concatenate of all hidden states corresponding with each time step.
</span>        <span class="c1"># outputs shape: (max_length , batch_size , hidden_size x num_directions)
</span>        <span class="c1"># hidden shape: (n_layers x num_directions , batch_size , hidden_size)
</span>        <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">packed</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
        <span class="c1"># Unpack padding. Revert of pack_padded_sequence
</span>        <span class="c1"># outputs shape: (max_length , batch_size , hidden_size x num_directions)
</span>        <span class="n">outputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">pad_packed_sequence</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="c1"># Sum bidirectional GRU outputs to reshape shape into (max_length , batch_size , hidden_size)
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">outputs</span><span class="p">[:,</span> <span class="p">:</span> <span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">:]</span>
        <span class="c1"># outputs shape:(max_length , batch_size , hidden_size)
</span>        <span class="c1"># hidden shape: (n_layers x num_directions , batch_size , hidden_size)
</span>        <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Để kiểm nghiệm kết quả trả ra của EncoderRNN bên dưới ta sẽ thực hiện 1 ví dụ giả lập encoder với <code class="highlighter-rouge">hidden_size</code> = 3 và <code class="highlighter-rouge">n_layers = 7</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="c1"># Thử nghiệm phrase Encoder bằng cách giả lập 1 mạng Encoder với:
</span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n_layers</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">voc</span><span class="o">.</span><span class="n">num_words</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'input_seq: </span><span class="se">\n</span><span class="s">'</span><span class="p">,</span> <span class="n">input_variable</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'input_lengths: </span><span class="se">\n</span><span class="s">'</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">EncoderRNN</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'encoder phrase: </span><span class="se">\n</span><span class="s">'</span><span class="p">,</span> <span class="n">encoder</span><span class="p">)</span>

<span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_seq</span> <span class="o">=</span> <span class="n">input_variable</span><span class="p">,</span> <span class="n">input_lengths</span> <span class="o">=</span> <span class="n">lengths</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre>input_seq: 
 tensor([[  66,  369,   66, 1272],
        [ 567,  183,   28,  616],
        [ 392, 1558, 1143,  175],
        [ 394,   31,   31, 5558],
        [   2,    2,    2,    2]])
input_lengths: 
 tensor([5, 5, 5, 5])
encoder: 
 EncoderRNN(
  (embedding): Embedding(171775, 3)
  (gru): GRU(3, 3, num_layers=7, bidirectional=True)
)
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="k">print</span><span class="p">(</span><span class="s">'output size: '</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">'hidden size: '</span><span class="p">,</span> <span class="n">hidden</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>output size:  torch.Size([5, 4, 3])
hidden size:  torch.Size([14, 4, 3])
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Ta nhận thấy đầu ra của output là <code class="highlighter-rouge">(max_length, batch_size, hidden_size)</code> và của hidden là <code class="highlighter-rouge">(n_layers x num_directions, batch_size, hidden_size)</code>. Thông qua ví dụ này bạn đọc đã hình dung được quá trình Encoder rồi chứ.</p>

<h2 id="22-decoder">2.2. Decoder</h2>
<p>Như vậy sau bước encoder ta thu được output là layer GRU cuối cùng (gọi là encoder outputs) và các hidden state của layer GRU cuối cùng. Bước tiếp theo chúng ta cần giải mã các kết quả thu được từ encoder thành câu hoàn chỉnh.</p>

<h3 id="221-áp-dụng-attention-layer">2.2.1. Áp dụng Attention layer</h3>
<p>Ở phrase này chúng ta sẽ sử dụng thêm 1 layer attention để tính phân phối trọng số attention weights cho các véc tơ từ (hidden state) của ma trận encoder outputs. attention weight sẽ có kích thước bằng đúng độ dài của câu. Sau khi tính tổng theo attention weights ta sẽ thu được context véc tơ đại diện cho toàn bộ câu. Quá hình này được thể hiện như hình 5. context véc tơ sẽ kết hợp với các decoder hidden state (các thẻ $h_t$ màu đỏ trong hình 5) tại mỗi time step để dự đoán từ tiếp theo. decoder hidden state chính là outptut trả ra của model Decoder sau mỗi time step. Quá trình này lặp lại truy hồi (output layer trước làm input cho layer sau) cho đến khi kết quả trả về là <code class="highlighter-rouge">&lt;EOS&gt;</code>  (end of sequence).</p>

<p>Hình bên dưới sẽ minh họa cho quá trình tính toán attention weigths dựa trên sự kết hợp của decoder input (véc tơ embedding của từ được dự báo ở bước trước), decoder hidden state (ma trận của các hidden state sau cùng) và encoder outputs.</p>

<p><img src="https://pytorch.org/tutorials/_images/attn2.png" width="600px" style="display:block; margin-left:auto; margin-right:auto" /></p>

<blockquote>
  <p><strong>Hình 3:</strong> Kết hợp giữa decoder input, decoder hidden state và encoder outputs để tạo ra một attended encoder outputs.</p>
</blockquote>

<p>Các bạn đã hình dung được quá trình kết hợp attention vào decoder rồi chứ? Điểm mấu chốt là chúng ta phải tính ra được các trọng số attention tại mỗi time step. Việc tính toán attention weight đã được đề xuất theo rất nhiều cách khác nhau bởi anh <a href="https://arxiv.org/abs/1508.04025">Lương Mạnh Thắng</a>. Trong đó điểm cải tiến so với cha đẻ của attention layer Bahdanau chính là một <code class="highlighter-rouge">global attention</code> được tính toán trên toàn bộ encoder hidden state thay vì <code class="highlighter-rouge">local attention</code> được tính toán dựa trên chỉ encoder hidden state của time step hiện tại. Điểm khác biệt thứ 2 là global attention tính attention weight chỉ dựa trên decoder hidden state tại time step hiện tại thay vì <code class="highlighter-rouge">local attention</code> được đề xuất bởi Bahdanau yêu cầu thêm các decoder hidden state trước đó.  Theo đó điểm của các attention ở time step hiện tại được anh Thắng Lương đề xuất dưới nhiều công thức khác nhau như hình bên dưới.</p>

<p><img src="https://pytorch.org/tutorials/_images/scores.png" width="400px" style="display:block; margin-left:auto; margin-right:auto" /></p>

<blockquote>
  <p><strong>Hình 4:</strong> Tính điểm tại time step t dựa trên decoder hidden state ($h_t$) của thời điểm $t$ và toàn bộ các encoder hidden states ($\bar{h_s}$).</p>
</blockquote>

<p>Các bước tiếp theo để tính context véc tơ ta có thể tham khảo ở bài diễn giải về <a href="https://phamdinhkhanh.github.io/2019/06/18/AttentionLayer.html">attention is all you need</a>.</p>

<p>Cơ chế tính context véc tơ có thể khái quát hóa như hình bên dưới. Lưu ý chúng ta sẽ triển khai <code class="highlighter-rouge">Attention Layer</code> như một <code class="highlighter-rouge">nn.Module</code> tách biệt và được gọi là <code class="highlighter-rouge">Attn</code>. Kết quả của attention layer là <code class="highlighter-rouge">attention weights</code> là một phân phối xác xuất hàm softmax dưới dạng tensor có kích thước <code class="highlighter-rouge">(batch_size, 1, max_length)</code>.</p>

<p><img src="https://pytorch.org/tutorials/_images/global_attn.png" width="600px" style="display:block; margin-left:auto; margin-right:auto" /></p>
<blockquote>
  <p><strong>Hình 5:</strong> Cơ chế global attention.</p>
</blockquote>

<p>Bên dưới là code của class <code class="highlighter-rouge">Attention</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
</pre></td><td class="rouge-code"><pre><span class="c1"># Luong attention layer
</span><span class="k">class</span> <span class="nc">Attn</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">method</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Attn</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s">'dot'</span><span class="p">,</span> <span class="s">'general'</span><span class="p">,</span> <span class="s">'concat'</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">method</span><span class="p">,</span> <span class="s">"is not an appropriate attention method."</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s">'general'</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s">'concat'</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">dot_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="c1"># encoder_output shape:(max_length , batch_size , hidden_size)
</span>        <span class="c1"># hidden shape: (1 , batch_size , hidden_size)
</span>        <span class="c1"># return shape: (max_length, batch_size)
</span>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">hidden</span> <span class="o">*</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">general_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="c1"># encoder_output shape:(max_length , batch_size , hidden_size)
</span>        <span class="c1"># hidden shape: (batch_size , hidden_size)
</span>        <span class="c1"># energy shape: (max_length , batch_size , hidden_size)
</span>        <span class="c1"># return shape: (max_length , batch_size)
</span>        <span class="n">energy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">encoder_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">hidden</span> <span class="o">*</span> <span class="n">energy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">concat_score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">):</span>
        <span class="c1"># encoder_output shape:(max_length , batch_size , hidden_size)
</span>        <span class="c1"># hidden shape: (batch_size , hidden_size)
</span>        <span class="c1"># energy shape: (max_length , batch_size , 2*hidden_size)
</span>        <span class="c1"># self.v shape: (hidden_size)
</span>        <span class="c1"># return shape: (max_length , batch_size)
</span>        <span class="n">energy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">hidden</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">encoder_output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">encoder_output</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">*</span> <span class="n">energy</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">):</span>
        <span class="c1"># Calculate the attention weights (energies) based on the given method
</span>        <span class="c1"># attn_energies.shape: (max_length , batch_size)
</span>        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s">'general'</span><span class="p">:</span>
            <span class="n">attn_energies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">general_score</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s">'concat'</span><span class="p">:</span>
            <span class="n">attn_energies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat_score</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">method</span> <span class="o">==</span> <span class="s">'dot'</span><span class="p">:</span>
            <span class="n">attn_energies</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dot_score</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>

        <span class="c1"># Transpose max_length and batch_size dimensions
</span>        <span class="c1"># attn_energies.shape: (batch_size , max_length)
</span>        <span class="n">attn_energies</span> <span class="o">=</span> <span class="n">attn_energies</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="c1"># Return the softmax normalized probability scores (with added dimension)
</span>        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_energies</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># attn_weights shape: (batch_size , 1 , max_length)
</span>        <span class="k">return</span> <span class="n">attn_weights</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Để hiểu rõ hơn attention hoạt động như thế nào ta sẽ cùng thử nghiệm truyền vào decoder hidden là các hidden state véc tơ của decoder và encoder outputs là ma trận mã hóa đầu ra của toàn bộ các từ trong câu input. Ví dụ bên dưới ta sẽ xét ở time step 0 nên decoder hidden khi đó sẽ là véc tơ hidden state cuối cùng của encoder (tham số last_hidden_encoder).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="n">method</span> <span class="o">=</span> <span class="s">'dot'</span>
<span class="c1"># Take last hidden encoder vector as a initialize of decoder
</span><span class="n">last_hidden_encoder</span> <span class="o">=</span> <span class="n">hidden</span><span class="p">[</span><span class="n">encoder</span><span class="o">.</span><span class="n">n_layers</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'last_hidden_encoder.size: '</span><span class="p">,</span> <span class="n">last_hidden_encoder</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">'encoder_outputs.size: '</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">'hidden_size: '</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="n">decoder_hidden_size</span> <span class="o">=</span> <span class="mi">7</span>
<span class="k">print</span><span class="p">(</span><span class="s">'decoder_hidden_size: '</span><span class="p">,</span> <span class="n">decoder_hidden_size</span><span class="p">)</span>

<span class="n">attn</span> <span class="o">=</span> <span class="n">Attn</span><span class="p">(</span><span class="n">method</span> <span class="o">=</span> <span class="n">method</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">decoder_hidden_size</span><span class="p">)</span>
<span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">last_hidden_encoder</span><span class="p">,</span> <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">output</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'attn_weights shape: '</span><span class="p">,</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre>last_hidden_encoder.size:  torch.Size([1, 4, 3])
encoder_outputs.size:  torch.Size([5, 4, 3])
hidden_size:  3
decoder_hidden_size:  7
attn_weights shape:  torch.Size([4, 1, 5])
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Như vậy tại mỗi time step để tính attention weights chúng ta sẽ truyền vào một decoder hidden véc tơ (chính là output của GRU tại time step đó) và ma trận encoder outputs. Thông qua các phương pháp dot_score, general_score hoặc concat_score ta sẽ tính được attention weights đại diện cho mức độ attention của từng vị trí của encoder outputs lên từ được dự báo.</p>

<p>Qua diễn giải trên chúng ta mới chỉ hiểu cách thức mà attention hoạt động. Vậy thì attention được áp dụng trong toàn bộ quá trình decoder như thế nào tại mỗi time step. Để hiểu rõ chúng ta tìm hiểu qua phrase decoder.</p>

<h3 id="222-quá-trình-decoder">2.2.2. Quá trình Decoder</h3>

<p>Trong bước decoder chúng ta sẽ truyền một từ một lần tại mỗi time step. Do đó các véc tơ embedding của từ và GRU output phải có chung shape là <code class="highlighter-rouge">(1, batch_size, hidden_size)</code>.</p>

<p><strong>Quá trình tính toán đồ thị</strong></p>
<ol>
  <li>Lấy embedding của input word hiện tại.</li>
  <li>thực hiện lan truyền thuận (feed forward) qua một GRU 1 chiều (unidirectional GRU) để thu được decoder hidden state.</li>
  <li>Tính attention weights từ decoder hidden state của GRU hiện tại ở bước 2 kết hợp với encoder outputs của encoder.</li>
  <li>Nhân attention weights với encoder outputs để thu được một tổng có trọng số là context véc tơ.</li>
  <li>Concatenate context véc tơ và GRU output.</li>
  <li>Dự báo từ tiếp theo (không sử dụng softmax).</li>
  <li>Trả về output và hidden state cuối cùng.</li>
</ol>

<p><strong>Inputs</strong></p>

<ul>
  <li><code class="highlighter-rouge">input_step</code>: một bước thời gian tương ứng với 1 từ của input sequence; shape = (1, batch_size).</li>
  <li><code class="highlighter-rouge">last_hidden</code>: layer GRU cuối cùng; shape=(n_layers x num_directions, batch_size, hidden_size).</li>
  <li><code class="highlighter-rouge">encoder_outputs</code>: đầu ra của bước encoder; shape = (max_length, batch_size, hidden_size).</li>
</ul>

<p><strong>Outputs</strong></p>

<ul>
  <li><code class="highlighter-rouge">output</code>: softmax normalized tensor trả về xác xuất tương ứng với mỗi từ là từ tại vị trí tương ứng của câu; shape=(batch_size, voc.num_words)</li>
  <li><code class="highlighter-rouge">hidden</code>: hidden state cuối cùng của GRU; shape=(n_layers x num_directions, batch_size, hidden_size). Do ở phrase decoder ta chỉ áp dụng unidirectional GRU nên shape = (n_layers , batch_size, hidden_size).</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">LuongAttnDecoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attn_model</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LuongAttnDecoderRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>

        <span class="c1"># Keep for reference
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">attn_model</span> <span class="o">=</span> <span class="n">attn_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>

        <span class="c1"># Define layers
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">n_layers</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">dropout</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">Attn</span><span class="p">(</span><span class="n">attn_model</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_step</span><span class="p">,</span> <span class="n">last_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">):</span>
        <span class="s">'''
        input_step: list time step index of batch. shape (1 x batch_size)
        last_hidden: last hidden output of hidden layer (we can take in right direction or left direction upon us) which have shape = (n_layers x batch_size x hidden_size)
        encoder_outputs: output of encoder 
        '''</span>
        <span class="c1">#===========================================
</span>        <span class="c1"># Step 1: Embedding current sequence index
</span>        <span class="c1"># Note: we run this one step (word) at a time
</span>        <span class="c1"># Get embedding of current input word
</span>        <span class="c1"># embedded shape: 1 x batch_size x hidden_size
</span>        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">input_step</span><span class="p">)</span>
        <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dropout</span><span class="p">(</span><span class="n">embedded</span><span class="p">)</span>
        
        <span class="c1">#===========================================
</span>        <span class="c1"># Step 2: pass embedded and last hidden into decoder
</span>        <span class="c1"># Forward through unidirectional GRU
</span>        <span class="c1"># rnn_output shape: 1 x batch_size x hidden_size
</span>        <span class="c1"># hidden shape: n_layers x batch_size x hidden_size
</span>        <span class="n">rnn_output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gru</span><span class="p">(</span><span class="n">embedded</span><span class="p">,</span> <span class="n">last_hidden</span><span class="p">)</span>
        <span class="c1"># Calculate attention weights from the current GRU output
</span>        <span class="c1"># attn_weights shape: batch_size x 1 x max_length
</span>        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">rnn_output</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
        <span class="c1"># Multiply attention weights to encoder outputs to get new "weighted sum" context vector
</span>        <span class="c1"># encoder_outputs shape: max_length x batch_size x hidden_size
</span>        <span class="c1"># context shape: batch_size x 1 x hidden_size
</span>        <span class="n">context</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">encoder_outputs</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="c1"># Concatenate weighted context vector and GRU output using Luong eq. 5
</span>        <span class="c1"># rnn_output shape: batch_size x hidden_size
</span>        <span class="n">rnn_output</span> <span class="o">=</span> <span class="n">rnn_output</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># context shape: batch_size x hidden_size
</span>        <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1">#===========================================
</span>        <span class="c1"># Step 3: calculate output probability distribution 
</span>        <span class="c1"># concat_input shape: batch_size x (2*hidden_size)
</span>        <span class="n">concat_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">rnn_output</span><span class="p">,</span> <span class="n">context</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># concat_output shape: batch_size x hidden_size
</span>        <span class="n">concat_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">concat_input</span><span class="p">))</span>
        <span class="c1"># Predict next word using Luong eq. 6
</span>        <span class="c1"># output shape: output_size
</span>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">concat_output</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># Return output and final hidden state
</span>        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Bên dưới ta sẽ giải thích quá trình dự đoán các từ tiếp theo tại time step đầu tiên. Lưu ý quá trình được thực hiện trên batch nên các đầu vào sẽ có kích thước scale theo batch_size.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td><td class="rouge-code"><pre><span class="n">time_step</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># Take all index of batch at time step 0. All words are &lt;SOS&gt; mark for start of sentences.
</span><span class="n">input_step</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">SOS_token</span><span class="p">]</span> <span class="o">*</span> <span class="n">small_batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">n_layers</span> <span class="o">=</span> <span class="mi">7</span>
<span class="c1"># take last hidden vector of encoder
</span><span class="n">last_hidden</span> <span class="o">=</span> <span class="n">hidden</span><span class="p">[:</span><span class="n">n_layers</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s">'batch_size: '</span><span class="p">,</span> <span class="n">small_batch_size</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'input_step.size at time_step 0: '</span><span class="p">,</span> <span class="n">input_step</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">'last_hidden.size: '</span><span class="p">,</span> <span class="n">last_hidden</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="n">attn_model</span> <span class="o">=</span> <span class="s">'dot'</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">3</span>
<span class="c1"># Output size of decoder model is size of vocabulary
</span><span class="n">output_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">voc</span><span class="o">.</span><span class="n">word2index</span><span class="p">)</span>


<span class="n">luongAttnDecoderRNN</span> <span class="o">=</span> <span class="n">LuongAttnDecoderRNN</span><span class="p">(</span><span class="n">attn_model</span> <span class="o">=</span> <span class="n">attn_model</span><span class="p">,</span>
                                         <span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">,</span>
                                         <span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span><span class="p">,</span>
                                         <span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">,</span>
                                         <span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'luongAttnDecoderRNN phrase: </span><span class="se">\n</span><span class="s">'</span><span class="p">,</span> <span class="n">luongAttnDecoderRNN</span><span class="p">)</span>
<span class="n">dec_output</span><span class="p">,</span> <span class="n">dec_hidden</span> <span class="o">=</span> <span class="n">luongAttnDecoderRNN</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">input_step</span> <span class="o">=</span> <span class="n">input_step</span><span class="p">,</span> 
                                                     <span class="n">last_hidden</span> <span class="o">=</span> <span class="n">last_hidden</span><span class="p">,</span> 
                                                     <span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">output</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre>batch_size:  4
input_step.size at time_step 0:  torch.Size([1, 4])
last_hidden.size:  torch.Size([7, 4, 3])
luongAttnDecoderRNN phrase: 
 LuongAttnDecoderRNN(
  (embedding): Embedding(171775, 3)
  (embedding_dropout): Dropout(p=0.1)
  (gru): GRU(3, 3, num_layers=7, dropout=0.1)
  (concat): Linear(in_features=6, out_features=3, bias=True)
  (out): Linear(in_features=3, out_features=171772, bias=True)
  (attn): Attn()
)
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="k">print</span><span class="p">(</span><span class="s">'dec_output.size: '</span><span class="p">,</span> <span class="n">dec_output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s">'dec_hidden.size: '</span><span class="p">,</span> <span class="n">dec_hidden</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>dec_output.size:  torch.Size([4, 171772])
dec_hidden.size:  torch.Size([7, 4, 3])
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>Diễn giải hàm forward:</strong></p>

<p>Như vậy sau khi truyền vào decoder embedding véc tơ đại diện cho từ liền trước kết hợp với last hidden output của chính time step trước (có kích thước = <code class="highlighter-rouge">n_layers x hidden_size</code>) ta sẽ thu được decoder hidden state véc tơ (chính là các rnn_output trong hàm <code class="highlighter-rouge">luongAttnDecoderRNN.forward()</code>). Đây chính là một quá trình lan truyền thuật thông thường của một mạng RNN.</p>

<p>Tiếp theo kết hợp decoder hidden state véc tơ với encoder outputs thu được ở phrase encoding ta sẽ tính được attention weight và từ đó suy ra context véc tơ. concatenate context véc tơ và decoder hidden state véc tơ tạo thành feature learning. Truyền feature learning véc tơ này qua hàm softmax ta sẽ tìm được véc tơ phân phối của từ tại vị trí time step.</p>

<h1 id="3-huấn-luyện-model">3. Huấn luyện model.</h1>
<h2 id="31-loss-function">3.1 Loss function</h2>

<p>Bởi vì chúng ta huấn luyện mô hình dựa trên các batch đã được padding nên không đơn thuần là đưa toàn bộ các phần tử của output vào để tính toán giá trị của loss function mà cần có thêm một binary mask tensor để xác định các vị trí padding của target tensor. Khi đó loss function được tính toán dựa trên decoder’s output tensor, target tensor và binary mask tensor. Hàm loss function chính là trung bình âm của log likelihood của toàn bộ các phần tử tương ứng với 1 trong 1 trong binary mask tensor.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">maskNLLLoss</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="n">nTotal</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">crossEntropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">crossEntropy</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">nTotal</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="32-huấn-luyện-vòng-lặp-đơn">3.2. Huấn luyện vòng lặp đơn</h2>

<p>Hàm huấn luyện <code class="highlighter-rouge">train()</code> bên dưới sẽ được xây dựng để huấn luyện cho 1 batch đơn lẻ.</p>

<p><strong>Các kĩ thuật sử dụng</strong></p>

<p>Chúng ta sẽ sử dụng 2 kĩ thuật để hỗ trợ hội tụ:</p>

<ul>
  <li>
    <p><strong>teacher forcing</strong>: Tại một ngưỡng xác xuất nào đó được thiết lập thông qua tham số <code class="highlighter-rouge">teacher_forcing_ratio</code>, chúng ta sẽ sử dụng luôn current target word như là đầu vào cho decoder tại bước tiếp theo hơn là sử dụng dự báo từ mô hình tại bước hiện tại. Kĩ thuật này hỗ trợ cho quá trình huấn luyện hiệu quả hơn. Tuy nhiên điểm hạn chế là có thể khiến cho mô hình thiếu ổn định trong quá trình suy diễn, khi decoder có thể không có cơ hội học đủ các khả năng để tạo ra output sequence trong quá trình huấn luyện. Do đó phải hết sức cẩn thận khi sử dụng <code class="highlighter-rouge">teacher_forcing_ratio</code>.</p>
  </li>
  <li>
    <p><strong>gradient clipping</strong>: Hay còn gọi là kĩ thuật kẹp gradient để tránh hiện tượng bùng nổ gradient (exploding gradient - gradient lớn quá nhanh) bằng các thiết lập giá trị max cho gradient descent.</p>
  </li>
</ul>

<p><strong>Chuỗi các bước triển khai</strong></p>

<ol>
  <li>Truyền vào toàn bộ các batch của câu qua encoder.</li>
  <li>Khởi tạo decoder input bằng token <code class="highlighter-rouge">&lt;SOS&gt;</code> (start of sequence) và encoder’s hidden state cuối cùng.</li>
  <li>Truyền input batch sequence vào decoder một lần tại một time step.</li>
  <li>Nếu thực hiện teaching forcing: Thiết lập decoder input tiếp theo chính là giá trị target hiện tại; trái lại: thiết lập decoder input tiếp theo như là đầu ra của decoder hiện tại.</li>
  <li>Tính loss function.</li>
  <li>Thực hiện lan truyền ngược.</li>
  <li>Kẹp gradient tránh hiện tượng bùng nổ tham số.</li>
  <li>Cập nhật các tham số của encoder và decoder cho mô hình.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">input_variable</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span>
          <span class="n">encoder_optimizer</span><span class="p">,</span> <span class="n">decoder_optimizer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">clip</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">):</span>

    <span class="c1"># Zero gradients
</span>    <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># Set device options
</span>    <span class="n">input_variable</span> <span class="o">=</span> <span class="n">input_variable</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">target_variable</span> <span class="o">=</span> <span class="n">target_variable</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Initialize variables
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">print_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n_totals</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Forward pass through encoder
</span>    <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">input_variable</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>

    <span class="c1"># Create initial decoder input (start with SOS tokens for each sentence)
</span>    <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="n">SOS_token</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]])</span>
    <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">decoder_input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Set initial decoder hidden state to the encoder's final hidden state
</span>    <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">encoder_hidden</span><span class="p">[:</span><span class="n">decoder</span><span class="o">.</span><span class="n">n_layers</span><span class="p">]</span>

    <span class="c1"># Determine if we are using teacher forcing this iteration
</span>    <span class="n">use_teacher_forcing</span> <span class="o">=</span> <span class="bp">True</span> <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">teacher_forcing_ratio</span> <span class="k">else</span> <span class="bp">False</span>

    <span class="c1"># Forward batch of sequences through decoder one time step at a time
</span>    <span class="k">if</span> <span class="n">use_teacher_forcing</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_target_len</span><span class="p">):</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span>
                <span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span>
            <span class="p">)</span>
            <span class="c1"># Teacher forcing: next input is current target
</span>            <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">target_variable</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># Calculate and accumulate loss
</span>            <span class="n">mask_loss</span><span class="p">,</span> <span class="n">nTotal</span> <span class="o">=</span> <span class="n">maskNLLLoss</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">mask</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">mask_loss</span>
            <span class="n">print_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">nTotal</span><span class="p">)</span>
            <span class="n">n_totals</span> <span class="o">+=</span> <span class="n">nTotal</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_target_len</span><span class="p">):</span>
            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span>
                <span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span>
            <span class="p">)</span>
            <span class="c1"># No teacher forcing: next input is decoder's own current output
</span>            <span class="n">_</span><span class="p">,</span> <span class="n">topi</span> <span class="o">=</span> <span class="n">decoder_output</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="n">topi</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]])</span>
            <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">decoder_input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="c1"># Calculate and accumulate loss
</span>            <span class="n">mask_loss</span><span class="p">,</span> <span class="n">nTotal</span> <span class="o">=</span> <span class="n">maskNLLLoss</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">mask</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">mask_loss</span>
            <span class="n">print_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">nTotal</span><span class="p">)</span>
            <span class="n">n_totals</span> <span class="o">+=</span> <span class="n">nTotal</span>

    <span class="c1"># Perform backpropatation
</span>    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Clip gradients: gradients are modified in place
</span>    <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip</span><span class="p">)</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">clip</span><span class="p">)</span>

    <span class="c1"># Adjust model weights
</span>    <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">print_losses</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_totals</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="33-huấn-luyện-vòng-lặp">3.3. Huấn luyện vòng lặp</h2>

<p>Cuối cùng đã đến lúc gắn kết toàn bộ quy trình huấn luyện với dữ liệu. Hàm <code class="highlighter-rouge">trainIters</code> chịu trách nhiệm chạy các <code class="highlighter-rouge">n_interations</code> quá trình huấn luyện từ các đầu vào gồm các model, hàm tối ưu optimizer, dữ liệu, v.v.</p>

<p>Một lưu ý là khi chúng ta lưu mô hình, chúng ta lưu vào một tarball (một dạng folder) chứa encoder và decoder state_dicts (chính là các tham số), optimizers’ state_dicts, the loss, iteration, …. Lưu mô hình theo cách này sẽ mang lại sự linh hoạt cho mô hình nhờ checkpoint. Sau khi load một checkpoint, chúng ta sẽ có thể sử dụng các tham số mô hình để chạy suy diễn hoặc có thể tiếp tục huấn luyện ngay tại trạng thái rời đi trước đó.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">trainIters</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> 
               <span class="n">encoder_optimizer</span><span class="p">,</span> <span class="n">decoder_optimizer</span><span class="p">,</span> 
               <span class="n">embedding</span><span class="p">,</span> <span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> 
               <span class="n">save_dir</span><span class="p">,</span> <span class="n">n_iteration</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">print_every</span><span class="p">,</span> 
               <span class="n">save_every</span><span class="p">,</span> <span class="n">clip</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="n">loadFilename</span><span class="p">):</span>
    <span class="s">'''
    model_name: Tên model
    voc: bộ từ vựng cho mô hình
    pairs: list các cặp (input, output)
    encoder: phrase encoder
    decoder: phrase decoder
    encoder_optimizer: phương pháp tối ưu hóa encoder
    decoder_optimizer: phương pháp tối ưu hóa decoder
    embedding: layer embedding
    encoder_n_layers: số lượng layers ở encoder
    decoder_n_layers: số lượng layers ở decoder
    save_dir: link save model
    n_iteration: số iteration tổng cộng được chọn ngẫu nhiên từ pairs. Mỗi iteration = 1 batch
    batch_size: kích thước của batch.
    print_every: số iteration của batch để in kết quả
    save_every: số iteration của batch để save model
    clip: trimming giá trị của tensor về 1 range
    corpus_name: tên bộ từ vựng
    loadFilename: link load file
    '''</span>
    <span class="c1"># Load batches for each iteration
</span>    <span class="n">training_batches</span> <span class="o">=</span> <span class="p">[</span><span class="n">batch2TrainData</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">pairs</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)])</span>
                      <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iteration</span><span class="p">)]</span>

    <span class="c1"># Initializations
</span>    <span class="k">print</span><span class="p">(</span><span class="s">'Initializing ...'</span><span class="p">)</span>
    <span class="n">start_iteration</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">print_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
        <span class="n">start_iteration</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s">'iteration'</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="c1"># Training loop
</span>    <span class="k">print</span><span class="p">(</span><span class="s">"Training..."</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_iteration</span><span class="p">,</span> <span class="n">n_iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">training_batch</span> <span class="o">=</span> <span class="n">training_batches</span><span class="p">[</span><span class="n">iteration</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
        <span class="c1"># Extract fields from batch
</span>        <span class="n">input_variable</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span> <span class="o">=</span> <span class="n">training_batch</span>

        <span class="c1"># Run a training iteration with batch
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">input_variable</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">target_variable</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">max_target_len</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span>
                     <span class="n">decoder</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">encoder_optimizer</span><span class="p">,</span> <span class="n">decoder_optimizer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">clip</span><span class="p">)</span>
        <span class="n">print_loss</span> <span class="o">+=</span> <span class="n">loss</span>

        <span class="c1"># Print progress
</span>        <span class="k">if</span> <span class="n">iteration</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">print_loss_avg</span> <span class="o">=</span> <span class="n">print_loss</span> <span class="o">/</span> <span class="n">print_every</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Iteration: {}; Percent complete: {:.1f}</span><span class="si">%</span><span class="s">; Average loss: {:.4f}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="n">iteration</span> <span class="o">/</span> <span class="n">n_iteration</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="n">print_loss_avg</span><span class="p">))</span>
            <span class="n">print_loss</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Save checkpoint
</span>        <span class="k">if</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">%</span> <span class="n">save_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">directory</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="s">'{}-{}_{}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">directory</span><span class="p">):</span>
                <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">directory</span><span class="p">)</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">({</span>
                <span class="s">'iteration'</span><span class="p">:</span> <span class="n">iteration</span><span class="p">,</span>
                <span class="s">'en'</span><span class="p">:</span> <span class="n">encoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s">'de'</span><span class="p">:</span> <span class="n">decoder</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s">'en_opt'</span><span class="p">:</span> <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s">'de_opt'</span><span class="p">:</span> <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                <span class="s">'loss'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>
                <span class="s">'voc_dict'</span><span class="p">:</span> <span class="n">voc</span><span class="o">.</span><span class="n">__dict__</span><span class="p">,</span>
                <span class="s">'embedding'</span><span class="p">:</span> <span class="n">embedding</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
            <span class="p">},</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">directory</span><span class="p">,</span> <span class="s">'{}_{}.tar'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="s">'checkpoint'</span><span class="p">)))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h1 id="4-xác-định-phương-pháp-đánh-giá">4. Xác định phương pháp đánh giá</h1>

<p>Sau khi huấn luyện mô hình, chúng ta đã có khả năng giao tiếp với bot. Muốn như vậy cần phải xác định xem chúng ta muốn mô hình mã hóa đầu vào như thế nào. Chúng ta có những cách giải mã sau:</p>

<p><strong>Mã hóa tham lam (greedy decoding)</strong>
Các mã hóa này được sử dụng trong tính huống không sử dụng có teacher forcing. Đầu ra của mô hình tại mỗi time step đơn giản là từ với xác xuất cao nhất.</p>

<p>Bên dưới chúng ta tạo ra class <code class="highlighter-rouge">GreedySearchDecoder</code> nhận đầu vào là một câu input với shape (input_seq length, 1), một scalar độ dài đầu vào (input_length) tensor, và một max_length để giới hạn độ dài lớn nhất của câu trả về. Câu đầu vào sẽ được đánh giá qua các bước như đồ thị bên dưới.</p>

<p><strong>Đồ thị tính toán</strong>:</p>

<ol>
  <li>Truyền input qua model encoder.</li>
  <li>Chuẩn bị hidden layer cuối cùng của encoder làm hidden input đầu tiên của decoder.</li>
  <li>Khởi tạo input đầu tiên của decoder là <code class="highlighter-rouge">&lt;SOS&gt;</code></li>
  <li>Khởi tạo tensor để append vào các từ được giải mã vào.</li>
  <li>Lặp lại quá trình giải mã một word token 1 lần:</li>
  <li>Truyền vào decoder.</li>
  <li>Thu được word token dự báo dựa trên xác xuất lớn nhất.</li>
  <li>Lưu lại token và score.</li>
  <li>Coi token hiện tại như là đầu vào tiếp theo của decoder.</li>
  <li>Trả về một tợp hợp các từ và điểm số.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">GreedySearchDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GreedySearchDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_seq</span><span class="p">,</span> <span class="n">input_length</span><span class="p">,</span> <span class="n">max_length</span><span class="p">):</span>
        <span class="c1"># Forward input through encoder model
</span>        <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">encoder_hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">input_length</span><span class="p">)</span>
        <span class="c1"># Prepare encoder's final hidden layer to be first hidden input to the decoder
</span>        <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="n">encoder_hidden</span><span class="p">[:</span><span class="n">decoder</span><span class="o">.</span><span class="n">n_layers</span><span class="p">]</span>
        <span class="c1"># Initialize decoder input with SOS_token
</span>        <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">long</span><span class="p">)</span> <span class="o">*</span> <span class="n">SOS_token</span>
        <span class="c1"># Initialize tensors to append decoded words to
</span>        <span class="n">all_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">long</span><span class="p">)</span>
        <span class="n">all_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># Iteratively decode one word token at a time
</span>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_length</span><span class="p">):</span>
            <span class="c1"># Forward pass through decoder
</span>            <span class="n">decoder_output</span><span class="p">,</span> <span class="n">decoder_hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="n">decoder_hidden</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>
            <span class="c1"># Obtain most likely word token and its softmax score
</span>            <span class="n">decoder_scores</span><span class="p">,</span> <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># Record token and score
</span>            <span class="n">all_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">all_tokens</span><span class="p">,</span> <span class="n">decoder_input</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">all_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">all_scores</span><span class="p">,</span> <span class="n">decoder_scores</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="c1"># Prepare current token to be next decoder input (add a dimension)
</span>            <span class="n">decoder_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">decoder_input</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># Return collections of word tokens and scores
</span>        <span class="k">return</span> <span class="n">all_tokens</span><span class="p">,</span> <span class="n">all_scores</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>Đánh giá câu dự báo</strong>
Qua bước trên chúng ta đã có hàm decoder xác định câu đầu ra dựa trên câu đầu vào. Hàm đánh giá quản lý quá trình xử lý câu đầu vào mức độ thấp. Trước tiên, chúng ta định dạng câu dưới dạng một batch đầu vào của các word index với batch_size = 1. Chúng ta thực hiện điều này bằng cách chuyển đổi các từ của câu thành các indexes tương ứng và transpose để chuẩn bị tensor cho các mô hình. Chúng ta cũng tạo ra một tensor độ dài chứa độ dài của câu đầu vào. Trong trường hợp này, độ dài là scalar vì chúng ta chỉ đánh giá một câu tại một thời điểm (batch_size = 1). Tiếp theo, chúng ta thu được một tensor các câu trả về được giải mã bằng cách sử dụng object <code class="highlighter-rouge">GreedySearchDecoder</code> (trình tìm kiếm). Cuối cùng, chúng tôi chuyển đổi các indexes trả về thành các từ và trả về danh sách các từ được giải mã.</p>

<p>class <code class="highlighter-rouge">evaluateInput</code> hoạt động như giao diện người dùng cho chatbot. Khi được gọi, một input text field sẽ sinh ra trong đó chúng ta có thể nhập câu hỏi của mình. Sau khi nhập câu đầu vào và nhấn Enter, text sẽ được chuẩn hóa giống như dữ liệu huấn luyện và cuối cùng được đưa vào hàm đánh giá để thu được câu đầu ra được giải mã. Chúng ta lặp lại quy trình này cho liên tục cho đến khi nhấn <code class="highlighter-rouge">q</code> hoặc <code class="highlighter-rouge">quit</code> để quit.</p>

<p>Cuối cùng, nếu một câu được nhập có chứa một từ không có trong từ vựng, chúng ta sẽ xử lý việc này một cách tinh tế bằng cách in một thông báo lỗi và nhắc người dùng nhập một câu khác.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_LENGTH</span><span class="p">):</span>
    <span class="c1">### Format input sentence as a batch
</span>    <span class="c1"># words -&gt; indexes
</span>    <span class="n">indexes_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">indexesFromSentence</span><span class="p">(</span><span class="n">voc</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)]</span>
    <span class="c1"># Create lengths tensor
</span>    <span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">indexes</span><span class="p">)</span> <span class="k">for</span> <span class="n">indexes</span> <span class="ow">in</span> <span class="n">indexes_batch</span><span class="p">])</span>
    <span class="c1"># Transpose dimensions of batch to match models' expectations
</span>    <span class="n">input_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="n">indexes_batch</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Use appropriate device
</span>    <span class="n">input_batch</span> <span class="o">=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Decode sentence with searcher
</span>    <span class="n">tokens</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">searcher</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
    <span class="c1"># indexes -&gt; words
</span>    <span class="n">decoded_words</span> <span class="o">=</span> <span class="p">[</span><span class="n">voc</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">decoded_words</span>


<span class="k">def</span> <span class="nf">evaluateInput</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">):</span>
    <span class="n">input_sentence</span> <span class="o">=</span> <span class="s">''</span>
    <span class="k">while</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Get input sentence
</span>            <span class="n">input_sentence</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s">'&gt; '</span><span class="p">)</span>
            <span class="c1"># Check if it is quit case
</span>            <span class="k">if</span> <span class="n">input_sentence</span> <span class="o">==</span> <span class="s">'q'</span> <span class="ow">or</span> <span class="n">input_sentence</span> <span class="o">==</span> <span class="s">'quit'</span><span class="p">:</span> <span class="k">break</span>
            <span class="c1"># Normalize sentence
</span>            <span class="n">input_sentence</span> <span class="o">=</span> <span class="n">normalizeString</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">)</span>
            <span class="c1"># Evaluate sentence
</span>            <span class="n">output_words</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">input_sentence</span><span class="p">)</span>
            <span class="c1"># Format and print response sentence
</span>            <span class="n">output_words</span><span class="p">[:]</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">output_words</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="s">'EOS'</span> <span class="ow">or</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'PAD'</span><span class="p">)]</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'Bot:'</span><span class="p">,</span> <span class="s">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_words</span><span class="p">))</span>

        <span class="k">except</span> <span class="nb">KeyError</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Error: Encountered unknown word."</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h1 id="5-huấn-luyện-model">5. Huấn luyện model</h1>

<p>Bên dưới ta sẽ thực hiện huấn luyện mô hình bằng cách thiết lập các tham số. Chúng ta có thể thử nghiệm nhiều tham số cấu hình khác nhau để lựa chọn được 1 mô hình tối ưu. Chúng ta cũng có thể xây dựng mô hình từ đầu hoặc load từ checkpoint một mô hình sẵn có.</p>

<p><strong>Load model</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
</pre></td><td class="rouge-code"><pre><span class="c1"># Configure models
</span><span class="n">model_name</span> <span class="o">=</span> <span class="s">'correct_spelling_model'</span>
<span class="n">corpus_name</span> <span class="o">=</span> <span class="s">'corpus_aivivn'</span>
<span class="n">attn_model</span> <span class="o">=</span> <span class="s">'dot'</span>
<span class="c1">#attn_model = 'general'
#attn_model = 'concat'
</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">encoder_n_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">decoder_n_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Set checkpoint to load from; set to None if starting from scratch
</span><span class="n">loadFilename</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">checkpoint_iter</span> <span class="o">=</span> <span class="mi">5000</span>
    
<span class="c1"># Load model if a loadFilename is provided
</span><span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
    <span class="c1"># If loading on same machine the model was trained on
</span>    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">loadFilename</span><span class="p">)</span>
    <span class="c1"># If loading a model trained on GPU to CPU
</span>    <span class="c1">#checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))
</span>    <span class="n">encoder_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s">'en'</span><span class="p">]</span>
    <span class="n">decoder_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s">'de'</span><span class="p">]</span>
    <span class="n">encoder_optimizer_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s">'en_opt'</span><span class="p">]</span>
    <span class="n">decoder_optimizer_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s">'de_opt'</span><span class="p">]</span>
    <span class="n">embedding_sd</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s">'embedding'</span><span class="p">]</span>
    <span class="n">voc</span><span class="o">.</span><span class="n">__dict__</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s">'voc_dict'</span><span class="p">]</span>


<span class="k">print</span><span class="p">(</span><span class="s">'Building encoder and decoder ...'</span><span class="p">)</span>
<span class="c1"># Initialize word embeddings
</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">voc</span><span class="o">.</span><span class="n">num_words</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="c1"># if loadFilename:
#     embedding.load_state_dict(embedding_sd)
# Initialize encoder &amp; decoder models
</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">EncoderRNN</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">LuongAttnDecoderRNN</span><span class="p">(</span><span class="n">attn_model</span><span class="p">,</span> <span class="n">embedding</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">voc</span><span class="o">.</span><span class="n">num_words</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
<span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
    <span class="n">encoder</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">encoder_sd</span><span class="p">)</span>
    <span class="n">decoder</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">decoder_sd</span><span class="p">)</span>
<span class="c1"># Use appropriate device
</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Models built and ready to go!'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>Building encoder and decoder ...
Models built and ready to go!
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Khi muốn load một model từ check point chúng ta chỉ cần thay đổi loadFilename như bên dưới.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># Khi muốn load model thì enable đoạn dưới để tạo file lưu địa model training.  
</span><span class="n">loadFilename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span>
                           <span class="s">'{}-{}_{}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">),</span>
                           <span class="s">'{}_checkpoint.tar'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">checkpoint_iter</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="51-huấn-luyện-model">5.1. Huấn luyện model</h2>

<p>Để huấn luyện mô hình trước tiên ta cần thiết lập các tham số. Gọi vào hàm <code class="highlighter-rouge">trainIters</code> để huấn luyện qua các vòng lặp.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre></td><td class="rouge-code"><pre><span class="c1"># Configure training/optimization
</span><span class="n">clip</span> <span class="o">=</span> <span class="mf">50.0</span>
<span class="n">teacher_forcing_ratio</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">decoder_learning_ratio</span> <span class="o">=</span> <span class="mf">5.0</span>
<span class="n">n_iteration</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">print_every</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">save_every</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Ensure dropout layers are in train mode
</span><span class="n">encoder</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1"># Initialize optimizers
</span><span class="k">print</span><span class="p">(</span><span class="s">'Building optimizers ...'</span><span class="p">)</span>
<span class="n">encoder_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
<span class="n">decoder_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">decoder_learning_ratio</span><span class="p">)</span>
<span class="k">if</span> <span class="n">loadFilename</span><span class="p">:</span>
    <span class="n">encoder_optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">encoder_optimizer_sd</span><span class="p">)</span>
    <span class="n">decoder_optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">decoder_optimizer_sd</span><span class="p">)</span>

<span class="c1"># Run training iterations
</span><span class="k">print</span><span class="p">(</span><span class="s">"Starting Training!"</span><span class="p">)</span>
<span class="n">trainIters</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">voc</span><span class="p">,</span> <span class="n">pairs</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">encoder_optimizer</span><span class="p">,</span> <span class="n">decoder_optimizer</span><span class="p">,</span>
           <span class="n">embedding</span><span class="p">,</span> <span class="n">encoder_n_layers</span><span class="p">,</span> <span class="n">decoder_n_layers</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">,</span> <span class="n">n_iteration</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span>
           <span class="n">print_every</span><span class="p">,</span> <span class="n">save_every</span><span class="p">,</span> <span class="n">clip</span><span class="p">,</span> <span class="n">corpus_name</span><span class="p">,</span> <span class="n">loadFilename</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre>Building optimizers ...
Starting Training!
Initializing ...
Training...
Iteration: 100; Percent complete: 2.0%; Average loss: 6.7370
Iteration: 200; Percent complete: 4.0%; Average loss: 5.0068
Iteration: 300; Percent complete: 6.0%; Average loss: 3.5866
Iteration: 400; Percent complete: 8.0%; Average loss: 2.3804
Iteration: 500; Percent complete: 10.0%; Average loss: 1.8249
Iteration: 600; Percent complete: 12.0%; Average loss: 1.5482
Iteration: 700; Percent complete: 14.0%; Average loss: 1.3622
Iteration: 800; Percent complete: 16.0%; Average loss: 1.2253
Iteration: 900; Percent complete: 18.0%; Average loss: 1.1341
Iteration: 1000; Percent complete: 20.0%; Average loss: 1.0986
Iteration: 1100; Percent complete: 22.0%; Average loss: 1.0536
Iteration: 1200; Percent complete: 24.0%; Average loss: 1.0119
Iteration: 1300; Percent complete: 26.0%; Average loss: 0.9666
Iteration: 1400; Percent complete: 28.0%; Average loss: 0.9291
Iteration: 1500; Percent complete: 30.0%; Average loss: 0.9200
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="52-đánh-giá-model">5.2 Đánh giá model</h2>

<p>Bên dưới chúng ta cùng đánh giá mô hình thông qua việc dự đoán một số từ không dấu. Lưu ý rằng do dữ liệu được huấn luyện chỉ là các ngram có 4 từ nên chúng ta sẽ truyền vào các cụm 4 từ. Để thêm dấu cho 1 câu văn với độ dài tùy ý bạn đọc có thể chia câu thành các ngram với kích thước 4 và ghép các kết quả dự báo trên từng ngram đơn lẻ. Hàm này tôi sẽ không viết ở đây và xin dành cho bạn đọc.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="c1"># Set dropout layers to eval mode
</span><span class="n">encoder</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>
<span class="n">decoder</span><span class="o">.</span><span class="nb">eval</span><span class="p">()</span>

<span class="c1"># Initialize search module
</span><span class="n">searcher</span> <span class="o">=</span> <span class="n">GreedySearchDecoder</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">)</span>

<span class="c1"># Begin chatting (uncomment and run the following line to begin)
</span><span class="n">evaluateInput</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">searcher</span><span class="p">,</span> <span class="n">voc</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>&gt; tai nguyen thien nhien
Bot: tài nguyên thiên nhiên
&gt; thuong mai dien tu
Bot: thương mại điện tử
&gt; hoc sinh cap 1
Bot: học sinh cấp 1
</pre></td></tr></tbody></table></code></pre></div></div>

<h1 id="6-hạn-chế-của-mô-hình">6. Hạn chế của mô hình</h1>

<p>Mô hình seq2seq luôn tồn tại một số hạn chế nhất định mà chúng ta sẽ nhận ra trong quá trình huấn luyện đó là:</p>

<ul>
  <li>
    <p>Mô hình rất tốn tài nguyên và thời gian huấn luyện lâu. Để huấn luyện mô hình dịch máy, google đã huy động một hệ thống máy chủ mà diện tích có thể trải trên 1 km2.</p>
  </li>
  <li>
    <p>Mô hình sẽ không thêm dấu chính xác đối với những cụm từ mà chúng chưa từng được học. Do đó để nâng cao mức độ chuẩn xác ngoài cần một kiến trúc mô hình mạnh thì một tập dữ liệu đủ lớn.</p>
  </li>
  <li>
    <p>Trong bài tôi sử dụng mô hình theo word level. Chính vì thế kích thước của vocabolary là rất lớn và sẽ ảnh hưởng đến số lượng parameter của model.</p>
  </li>
  <li>
    <p>Huấn luyện mô hình theo character level sẽ có lợi thế hơn khi các từ thay đổi chỉ nằm trong tập các chữ cái <code class="highlighter-rouge">ueoaidy</code>. Do đó chúng ta sẽ teacher forcing để chỉ thay đổi các từ nằm trong tập <code class="highlighter-rouge">ueoaidy</code> và không thay đổi các từ còn lại.</p>
  </li>
  <li>
    <p>Mô hình mới chỉ xây dựng cho các cụm từ ngram với kích thước bằng 4. Để dự báo cho câu với độ dài tùy ý dựa trên ngram = 4 xin dành cho bạn đọc.</p>
  </li>
  <li>
    <p>Lớp model transformer cũng là một trong những mô hình cân nhắc thay thế cho seq2seq trong bài toán này vì tốc độ tính toán nhanh, có thể xử lý trên nhiều GPU. Bạn đọc có thể tham khảo ý tưởng của đội xếp 2nd của cuộc thi thêm dấu tiếng việt.</p>
  </li>
</ul>

<h1 id="7-tài-liệu-tham-khảo">7. Tài liệu tham khảo</h1>

<ol>
  <li><a href="https://phamdinhkhanh.github.io/2019/08/10/PytorchTurtorial1.html">hướng dẫn pytorch</a></li>
  <li><a href="https://phamdinhkhanh.github.io/2019/04/22/L%C3%BD_thuy%E1%BA%BFt_v%E1%BB%81_m%E1%BA%A1ng_LSTM.html">lý thuyết về mạng LSTM</a></li>
  <li><a href="https://pytorch.org/tutorials/beginner/chatbot_tutorial.html">seq2seq pytorch</a></li>
  <li><a href="https://github.com/tensorflow/tensor2tensor">tensor2tensor project</a></li>
  <li><a href="https://phamdinhkhanh.github.io/2019/06/18/AttentionLayer.html">attention là tất cả bạn cần</a></li>
  <li><a href="https://forum.machinelearningcoban.com/t/aivivn-3-vietnamese-tone-prediction-1st-place-solution/5721">thêm dấu cho tiếng việt - aivivn 1st</a></li>
  <li><a href="https://forum.machinelearningcoban.com/t/aivivn-3-vietnamese-tone-prediction-2nd-place-solution/5759">thêm dấu cho tiếng việt - aivivn 2nd</a></li>
</ol>

<script data-ad-client="ca-pub-4263248182804679" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<button onclick="topFunction()" id="myBtn" title="Go to top">Top</button>

<script src="/js/toc.js"></script>
<script src="/js/btnTop.js"></script>
<script type="text/javascript">
$(document).ready(function() {
    $('#toc').toc();
});
</script>
				</div>
			</div>
			<div class="col-md-2 hidden-xs hidden-sm">
				<a  href="/">
					<img width="100%" style="padding-bottom: 3mm;" src="/assets/images/logo.jpg" /> </a>
				<br>
				<nav>
					<div class="header">Khanh's site</div>
					<li><a style="text-align: left; color: #074B80"  href="https://www.facebook.com/TowardDataScience">phamdinhkhanh blog</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://www.facebook.com/groups/3235479620010379/">phamdinhkhanh AICode forum</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://kaggle.com/phamdinhkhanh">phamdinhkhanh kaggle</a></li>
					<li><a style="text-align: left; color: #074B80"  href="http://rpubs.com/phamdinhkhanh">phamdinhkhanh rpub</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://github.com/phamdinhkhanh">phamdinhkhanh github</a></li>
					<br>
					<div class="header">other's site</div>
					<li><a style="text-align: left; color: #074B80"  href="https://www.facebook.com/groups/machinelearningcoban/">machine learning cơ bản facebook</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://forum.machinelearningcoban.com/">machine learning cơ bản forum</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://machinelearningmastery.com">machine learning mastery</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://viblo.asia">viblosia</a></li>
					<br>
					<div class="header">Khóa học</div>
					<li><a style="text-align: left; color: #074B80"  href="http://web.stanford.edu/class/cs109/">Xác suất thống kê(Probability): CS109</a></li>
					<li><a style="text-align: left; color: #074B80"  href="http://web.stanford.edu/class/cs246/">Bigdata: CS246</a></li>
					<li><a style="text-align: left; color: #074B80"  href="http://cs231n.stanford.edu/">Computer vision cơ bản: CS231N</a></li>
					<li><a style="text-align: left; color: #074B80"  href="http://web.stanford.edu/class/cs224n/">Natural Language Processing: CS224N</a></li>
					<li><a style="text-align: left; color: #074B80"  href="http://web.stanford.edu/class/cs224w/">Khoá phân tích mạng lưới (analysis of network): CS224W</a></li>
					<li><a style="text-align: left; color: #074B80"  href="https://web.stanford.edu/class/cs20si/">Khóa học Tensorflow: CS20SI</a></li>
				</nav>
			</div>
		</div>
	</div>
	
</body>
</html>
